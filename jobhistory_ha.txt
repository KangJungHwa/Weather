#############################################################
# nohup 명령어 &
#############################################################
&는 백그라운드로 명령을 실행한다.
그런지만 로그아웃을 하게되면 프로세스가 종료가 된다.
nuhup을 사용하면 프로세스가 종료되지 않는다.
데몬처럼 실행되는 명령어 이다.

아래와 같이 &를 2개 붙이면 하나 성공적으로 완료후 다음작업을 실행한다.
./run.sh && ./run1.sh

이미 실행된 명령어를 백그라운드로 보내기
ctrl + Z를 눌러서 잠깐 빠져나온다.
$>bg 를 실행햐면 백그라운드로 전환된다.
$>fg 는 백그라운드의 명령을 포그라운드로 전환한다.



#############################################################
# ps -fu $USER
#############################################################
해당 유저가 실행한 프로세스를 조회한다.
ps -fu $USER


jobs 백그라운드 작업의 상태를 나타내준다.
포그라운드의 작업을 백그라운드로 옮기기
bg, fg 명령어 조사해 보기

#############################################################
# 쉘스크립트 자체변수
#############################################################
$$ : 쉘자체의 pid
$* : 모든 매개변수
$@ : 모든 매개변수 IFS에 영향을 받지 않음
$0 : 실행된 쉘파일 이름
$1~$2 : 위치파라메터
$? : 이전에 실행했던 명령어의 exit code
$# : 파라메터의 전체 갯수

#############################################################
# 해당변수에 값이 없을때만 할당하는 방법
# :
#############################################################
export $(JAVA_HOME:=/usr/java/default)
cp large_file.tar $(TEMP_DIR:=/tmp)

#############################################################
# 퍼미션 변경 스크립트
#############################################################
function setperm {
  chown $1:$2 $1.*.keytab
  chmod 600 $1.*.keytab
}


#############################################################
# tar 명령
#############################################################
묶기
   tar cvf test.tar test*
풀기
   tar xvf test.tar
위치를 지정해서 풀기
   tar xvfz test.tar.gz -C /tmp/tar_test/
타르파일 내용보기
   tar tvf test.tar
퍼미션 유지하며 tar 묶기
   tar cvfp test.tar test*
퍼미션 유지하며 tar 풀기
   tar xvfp test.tar
기존 tar파일에 새파일 추가하여 묶기
   tar rvf test.tar test.txt
tar파일 내에 파일 교체 하기
   tar uvf test.tar test1.txt test2.txt
tar파일 내에 파일 삭제 하기
   tar --delete --file test1.txt test2.txt
tar파일 내에 특정파일만 풀기
   tar --extract --file test1.txt
tar는 분할 압축(2g이상은 묶이지 않기 때문에 분할을 해야 한다.)
   tar cvfpz - /home | split -b 1024mb - test.tar
tar는 분할 압축 해제
   cat citiconda2.tar* | tar xvfz -
Tar 분할압축 해제 폴더 지정   
   cat citiconda2.tar* | (cd /tmp;tar xvfz -)
Tar에서 절대경로를 사용하고 싶을 때는 대문자 P옵션을 줘야 한다.   
   tar cvfzP /tmp/en_txt.tar /tmp/en_txt/*
#############################################################   
# 특정디렉토리에서 파일을 찾아서 tar로 묶기
#############################################################
  step 1: 먼저 기본이 될 tar파일 하나를 만든다.
     tar -cvf test.tar test.txt
  step 2: for문에서 find로 찾은 파일을 기본 tar에 추가한다.	 
     for i in $(find . -name '*.txt')
	    do
		  tar rvf test.tar $id
	 done
	 
#############################################################
# passwordless ssh를 이용하여 여러서버에 명령실행
# passwordless ssh를 이용하여 여러서버에 파일 복사
#############################################################
#!/bin/bash
while read line
  do
    ssh -n $line 'ls -al /tmp/'
done < servers.txt

#!/bin/bash
for hostnm in bdicr101x{07..16}h2
do 
scp $1 $hostnm:$2
done

#!/bin/bash
for hostnm in bdicr101x0{7..16}h2
  do
    ssh -n hostnm $1
done
#############################################################
#scp로 다운로드 하기
#############################################################
scp <계정>@<서버>:/<받을파일> <받을디렉토리>


#############################################################
# 로컬의 쉘스크립트를 원격의 호스트에서 실행하기
#############################################################

#!/bin/bash
for hostnm in bdicr101x0{7..16}h2
  do
    cat test.sh | ssh -n hostnm "sh"
done


ps -ef
#############################################################
# 무한루프 
#############################################################
while :
  do 
     echo "press ctrl + C to stop"
	 sleep 5
  done
  
for (( ; ;))
  do 
     echo "press ctrl + C to stop"
  done  
  


#############################################################
# how to use echo option
#############################################################
echo -n 
  줄바꿈 없이 출력한다. 아래명령어는 줄바꿈없이 한줄로 출력됨
  echo -n `ls -al /tmp`
echo -e
  문자열 해석 기능이 있어 ls, df 등의 명령어를 
  변수에 담아 출력할때 메일 본문에사용한다. 
  
#############################################################
#  로그파일 만들때 처음 시작부분
#  banner
#############################################################
function banner() {
    echo "*********************************"
    log_it "*** Starting: `basename ${BASH_SOURCE[0]}` Version: $VERSION ***"
    echo "*********************************"
}
#############################################################
#  쉘스크립트 시작시 파라메터 갯수 체크
#############################################################
if [ ${#} -lt 2 ]; then
  print_usage
  exit 1
fi

#############################################################
#  패키지가 이미 설치되어 있으면 종료 시키기
#############################################################
if rpm -qa | grep -q $package_name; then
    die "Package $package_name is already installed. Uninstall it first" 0
fi
export IS_MYSQL=`yum list -q installed MySQL-server > /dev/null 2> /dev/null; echo $?`
if [ $IS_MYSQL -eq 0 ] ; then
    log_it "INFO: Already installed MySQL Server.  Skipping."
    ## Start service just to be sure
    log_it `service mysql start`

    ## Enable mysql service at boot
    /sbin/chkconfig mysql on

     exit 0
fi
#############################################################
#  --- Exit function
# ${2:--1} => 2번째 파라메터가 없을때는 exit -1
#############################################################
function die() {
    log_it "ERROR: ${1}"
    exit ${2:--1};
}
#############################################################
#  --- log_it function
#############################################################
function log_it () {
    echo "$@"
	echo "[$(date +"%Y%m%d %H%M%S")]: $@" > $LOG_FILE
    #echo "[$(date +"%D %T")]: $@" >> $LOG_FILE
}


#############################################################
#  로그파일이 있는지 체크후 로그파일 만들기
#  -z 문자열 length is zero인지 비교시
#  -n 문자열 null 비교시
#############################################################
if [ -z $LOG_FILE ]; then
  mkdir -p $LOG_DIR
  LOG_FILE=$LOG_DIR/mysql_install-$NOW.log
fi



##############################################################
# --- 파일의 존재여부를 체크하여 있으면 복사하기
&& 테스트 후 참이면 실행한다.
|| 테스트 후 거짓 이면 실행한다.
-d 디렉토리면 참
-f 파일이 존재하고 일반파일이면 참
-e 파일이 존재하면 참
-r 읽을수 있으면 참 -w 쓸수 있으면 참 -x 실행할 수 있으면 참
-s 파일이 존재하고 0바이트 보다 크면 참
-L 파일이 존재하고 링크파일이면 참
##############################################################
[ -d /tmp/test ] && mkdir /tmp/test2; cp -R /tmp/test/* /tmp/test2/
[ -f /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test
[ -e /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test2
[ -r /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test3
[ -w /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test4
[ -x /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test5 || echo "now exists"
[ -s /tmp/cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test6
[ -L /tmp/ln_cacerts ] && cp /tmp/cacerts /tmp/test/cacerts_test7

##############################################################
# --- 명령어 실행하고 정상적으로 실행되었는지 확인하기
##############################################################
service mysql start
if [ $? -ne 0 ]; then
   log_it "Could not start MySQL service"
   exit 1
fi

##############################################################
# case 문에서 or 조건 사용하기
##############################################################
host_type=$1
case $host_type in 
        MASTER|PROXY|EDGE|CM|DB)
            echo "We have a valid host type: $host_type"
        ;;
        *)
           echo "Wrong server type provided. Must be in MASTER, CM , DB or PROXY"
           exit 1
        ;;
esac

##############################################################
# case 문에서 or 조건 사용하기
# or 조건 전체를 []로 감싸줘야 한다.
##############################################################
host_type=$1
if [[ $host_type == "PROXY" || $host_type == "MASTER" || $host_type == "CM" ]]; then
    echo $host_type
elif [[ $host_type == "DB" ]]; then
    echo $host_type
fi

#############################################################
# yum 명령어 옵션
#############################################################
yum install -y : 패키지를 설치한다. 의존성있는 패키지까지 설치한다.
                 -y 옵션을 줄 경우 묻지 않고 바로 설치한다.
yum update <패키지명>   : 패키지를 업데이트한다 패키지명이 없는경우
                          전체시스템에서 update될 패키지를 확인한고
						  업데이트를 한다.
yum downgrade   :   downgrade a package
yum search      :   Search package details for the given string	
yum provides '*/httpd.conf':httpd.conf를 포함하고 있는 모든 패키지를 출력한다.
yum repolist : 현재 활성화되 repolist를 출력한다.
yum clean    :  Remove cached data
yum localinstall : 다운로드 없이 로컬에있는 rpm을 가지고 설치하는경우
                   rpm으로 설치할 수도 있지만 yum으로 설치하면
				   설치기록이 남아서 rollback이 가능한다.
yum history list all
                 : yume 사용기록을 모두 볼수 잇다.
yum history info <트랜젝션id>
ex) yum history info 5
                 : 해당 트랜젝션 id에 대해 상세히 볼때 사용한다.
--downloadonly :don't update, just download
                위의 옵션을 사용하기 위해서는 yum-downloadonly패키지를 설치해야 한다.
ex) yum install yum-downloadonly
--downloaddir  :specifies an alternate directory to store packages
-v, --verbose  :       verbose operation
--disablerepo=* : 레파지토리가 여러개 있는경우 disable 시킨다.
--enablerepo="soe-bigdata-c5" 
  : soe-bigdata-c5 를 enable 시킨다.
  repo파일 명을 적는 게 아니라 repo파일 내부의 대괄호 안의 문자를
  기술해준다.
--nogpgcheck : GPG 서명검증을 사용하지 않는다.
               해당 repo의 공개키가 없어서 실패할 경우 사용한다.

############################################################
# 현재 shell 스크립트가 실행되고 있는 경로 알아내기
############################################################
DIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )

############################################################
# getopts 사용
# while getopts "ap:" option : 
# 파라메터가 있는 옵션을 처리할때는 콜론(:)을 사용한다.
# "$OPTARG"  
# 파라메터가 있는 옵션에서 파라메터를 받는 시스템 변수다.
# \?)
# case 문에서 지정되지 않은 옵션을 처리할때 사용한다.
# if [ -n $seperator ]; then
# 문자열 비교시 null이 아닌때
# shift $(expr $OPTIND -1)하면 옵션 파라메터는 모두 삭제되고
# 위치파라메터를 $1으로 받을 수 있게 된다.
############################################################

#!/bin/bash
a_flag=0
seperator=""

while getopts "ap:" option
    do
       case $option in
       a)
         a_flag=1
        ;;
       p)
         seperator="$OPTARG"
        ;;
       \?)
         echo "invalid option"
         exit 1
       esac
    done

shift $(expr $OPTIND -1)

if [ $a_flag -eq 1 ]; then
  ls -al $1
  echo $seperator
else
  ls $1
  echo $seperator
fi


########################################
# 사용자의 입력을 처리하는 방법
#########################################
# -p 프롬프트를 얻는다.
# response 사용자의 입력이 저장되는 변수
# response=${response,,}
# 소문자로 변환한다.
#response=${response^^}
# 대문자로 변환한다.
# if [[ $response =~ (yes|y| ) ]]; then
# 여러개의 문자를 비교하기 위한 방법
########################################
read -r -p "Are you sure? [Y/n] " response
response=${response,,} # to lower
if [[ $response =~ (yes|y| ) ]]; then
    echo "your response is yes"
elif [[ $response =~ (no|n| ) ]]; then
    echo "your response is no"
else
    echo "your response is $response"
fi

########################################
# how to use cp option
########################################
원본파일 존재시 원본 복사본 만들고 복사.
마지막에 "~" 붙어서 생성됨
   cp -b /tmp/fetch.err /tmp/dir1
원본파일 존재시 원본을 지우고 복사
   cp -f /tmp/fetch.err /tmp/dir1
원본디렉토리에 하위폴더 존재시 폴더까지 복사.
   cp -R /tmp/dir1/ /tmp/dir2/
원본파일 복사시에 퍼미션 오너 시간정보를 보존하여 복사.
root로 실행할때만 퍼미션 오너 시간정보 보존 가능함.
   cp -p /tmp/dir1/ /tmp/dir2/
cp 명령어 실패시 exit 코드 숨기기
   cp /tmp/test.txt /tmp/test2.txt 2>/dev/null || :




########################################
# how to check string empty & length
########################################
var=null
if[ -n "$var" ]; then
  echo "var=null is zero length"
fi

if[ ! -z "$var" ]; then
  echo "var=null is empty"
fi

var=""
if[ -n "$var" ]; then
  echo "var=\"\" is zero length"
else
  echo "var=\"\" is not zero length"
  echo "var=\"\" is not empty"
fi

var="abcd"
if[ ! -z "$var" ]; then
  echo "var\'s length is ${#var}"
fi

########################################
# how to use curl option
########################################
curl -k
  https에서 다운로드 받을때 지정하는 옵션으로 --insecure와 동일한다.
curl -O
  파일을 다운로드 할때 리모트 파일명과 동일하게 파일 다운로드 한다.
  동시에 여러 사이트에서 다운받을땐 아래처럼 한다.
  curl -O $URL -O $URL -O $URL ....
curl -u
  해당사이트에 user:passward를 지정할때 사용한다.
  curl -Ou admin:Cmpassdev1 -k https://XXXXXXXXXXXXXXXX:7183/api/v9/clusters/KRBDDEV/services
curl -o 
  다른이름으로 파일을 저장하려면 소문자 -o옵션을 사용한다. 
  curl -o service.json -k https://XXXXXXXXXXXXXXXX:7183/api/v9/clusters/KRBDDEV/services 
curl --cacert  
  인증서를 사용하여 다운로드
  curl -Ou admin:Cmpassdev1 --cacert /opt/Cloudera/ssl/ca-cert.pem https://XXXXXXXXXXXXXXXX:7183/api/v9/clusters/KRBDDEV/services
curl -#  
  Display transfer progress as a progress bar 
curl -T 
  curl을 이용한 파일 업로드 
  cloudera manager web root 디렉토리 /usr/share/cmf/webapp/
  기본적으로 세팅에서 업로드가 안되게 되어 있음.
  curl -u admin:Cmpassdev1 -T /tmp/fetch.err -kv https://XXXXXXXXXXXXXXXX:7183/
curl -X
  사용할 http 메소드를 기술한다.(POST or GET)  
  curl -XGET http://xxxxxxxxxxxxxxxx:19890/jobhistory
curl -L
  페이지가 없어졌을때 redirection URL에서 받아온다.  
  
curl http://xxx.xxx.xxx.xxx/install.sh | ksh   
  curl을 이용해서 원격지의 쉘스크립트 실행하기

########################################
# how to use find option
########################################  
#tmp directory 에서 type이 디렉토리고 datameer인 디렉토리를 찾는다.
find /tmp/ -type d -name "datameer"
#tmp directory 에서 type이 파일이고 CertAndKey.pfx 파일을 찾는다.
find /tmp  -type f -name CertAndKey.pfx
#tmp directory 에서 변경된지 5분 미만의 파일 찾기
find /tmp -mmin -5

#tmp directory 에서 변경된지 3일(72시간) 전 보다 새로운
find /tmp -mtime -3
#tmp directory 에서 변경된지 4일(96시간 전에서 3일(72시간)전까지(하루치)
find /tmp -mtime 3
#tmp directory 에서 변경된지 4일(96시간)보다 오래된
find /tmp -mtime +3
#tmp directory 에서 변경된지 4일 전부터 변경된지 2일전까지 
find /tmp -mtime -4 -mtime +1

######################################################################
# 1년이상 오래된 logfile 삭제 하기
# f옵션은 파일이 없을때 에러발생을 방지한다.
# v는 삭제한 파일명을 화면에 표시한다.
# -0 옵션을 사용하면 파일명에 공백이 있어도 에러발생을 방지한다.
######################################################################
find /tmp -name "*.log" -mtime +364 -print | xargs -0 rm -fv
find ./ -name "test*.log" 2>/dev/null | xargs rm -rv
######################################################################
# log 파일을 찾아서 error이 있는 문자열만 출력
######################################################################
find ./ -name "test*.log" 2>/dev/null | xargs grep "error"

######################################################################
# 퍼미션을 이용한 파일의 검색
# -perm 644는 퍼미션이 정확히 644인 파일만 검색한다.
# -perm +001는 오너, 그룹, 아더중 1이상 일치하는 파일을 모두 찾는다.
# -perm +001는 other가 0이상인 파일을 찾는다.
# -perm -644는 오너, 그룹, 아더가 모두 지정된 퍼미션 이상인것을 찾는다.
######################################################################
find ./ -name "*.*" -perm 644
find ./ -name "*.*" -perm +001
find ./ -name "*.*" -perm -644

######################################################################
# 파일 소유권을 이용한 파일의 검색
# -user root 소유자가 root 파일을 검색
# -group root 소유그룹이 root 파일을 검색
######################################################################
find ./ -name "*.*" -user root
find ./ -name "*.*" -group root
 
######################################################################
# how to use awk option
1) Amit     Physics    80
2) Rahul    Maths      90
3) Shyam    Biology    87
4) Kedar    English    85
5) Hari     History    89
6) Demo1     User      58
7) test1    testdemo   78
######################################################################
파일을 읽어서 출력
  awk '{print}' /etc/hosts

파일을 읽어서 출력
  awk '/root/{print}' /etc/passwd
  awk '/root/' /etc/passwd

파일 시스템 use % 출력
  df -Ph | awk '{ print $5}'
선택한 컬럼만 추가
  df -Ph | awk '{ print $1, $3 }'
  awk '{print $3 " "  $4}' marks.txt
  awk '{print $3 "\t"  $4}' marks.txt
구분자를 지정해서 컬럼값 가져오기(-F:)
awk -F: '{ print $1 " " $3 }' < /etc/passwd
컬럼 간의 계산  
  df -P | awk  '{ print $1, $2 - $3}' | awk '{ print $1, $2/1024/1024}'
파일사이즈가 1 mbyte가 넘는 파일만 보기
  ls -al | awk '$5 > 1024567{print $3 " " $9}'    
Row Number 표시 ($0: 전체 컬럼)
  df -Ph | awk '{ print NR, $0 }'
패턴 필터링 (주석 처리된 row만(^))
  awk '/^#/{print}' /etc/hosts
패턴 필터링 (root로 시작하는 row만)
  awk '/^root/{print}' /etc/passwd
패턴 필터링 (bdyir문자열을 포함하는 row만)
  awk '/bdyir/{print}' server.txt
패턴 필터링 (x03h2로 시작하는줄)
  awk '/^19/{print}' /etc/hosts
패턴 필터링 (x03h2로 끝나는 줄($))  
  awk '/x03h2$/{print}' /etc/hosts
패턴 필터링 (install 문자열앞에 g,p,a 가 있는 모든 줄 
  ls -l | awk '/[gpa]install/ {print}'
패턴 필터링 (파일이든 디렉토리든 퍼미션이 600인 것들)
  d- 문자로 시작하고 rw-------인 퍼미션인 파일들
  ls -l | awk '/[d-]rw-------/ {print}'
패턴 필터링 (와일드 카드)
  ls -l | awk '/bdicr101x0.h2/{print}'  :
패턴 필터링 (숫자가 포함된 줄)  
  awk '/[0-9]/{print}' /etc/hosts  
패턴 필터링 (문자가 포함된 줄)  
  awk '/[a-Z]/{print}' /etc/hosts
  

############################################################
# AWK를 이용한 파일 시스템 감시
# awk -v threshold 변수값 지정
# NR==1 {next} 현재의 row가 1인경우 skip
# $6=="/xenv1" {next} 번째 컬럼이 /xenv1인경우 skip
# $6=="/xenv2" {next} 번째 컬럼이 /xenv1인경우 skip
# {sub(/%/,"",$5)} $5에서 /%/를 찾아서 ""로 대체한다.
# printf 서식 지정자 출력
#  %s 문자열 그대로 출력 
#  %d%%\n" 5번열을 출력할때 %를 결합해서 출력하고 줄바꿈을 하라
############################################################
threshold=30
message=$(df -Ph | awk -v threshold="$threshold" '
   NR==1 {next}
   $6=="/xenv1" {next}
   $6=="/xenv2" {next}
   {sub(/%/,"",$5)}
   $5 > threshold {printf "%s is almost full: %d%%\n", $6, $5}')
echo $message  
  
##############################################################
# awk를 이용해서 id 명령에서 uid와 id를 추출하기
##############################################################
echo `id|awk -F\( '{print $1}`
uid=79444917
echo `/usr/bin/id|awk -F\( '{print $1}'|awk -F= '{print $2}'`
79444917
echo `id|awk -F\( '{print $2}'|awk -F\) '{print $1}'`
gpainstall

##############################################################
# --- Must run as root
# 또는 whoami로 조회하는 방법과 id로 검색하는 방법이 있음
##############################################################
RUID=`/usr/bin/id|awk -F\( '{print $1}'|awk -F= '{print $2}'`
if [ $RUID -ne 0 ];then
    die  "You must be logged in as user with UID 0 (e.g. root user) to run $0 script." 1
fi
if [ `whoami` != "$INSTALL_ACCOUNT" ] ; then
  echo "ERROR: This script needs to be ran as $INSTALL_ACCOUNT user and it is being ran as: $(whoami)"
  exit 1
fi

############################################################################ 
# 환경설정 파일에서 속성에 매핑된 value 추출하기
# 환경설정 파일에는 cdh.parcel.version=5.7.0이라고 입력되어있음
# `cat config.ini|grep ^cdh.parcel.version|awk -F\= '{print $2}'`
# config.ini에서 grep으로 ^cdh.parcel.version로 시작하는 열을 검색하고
# 검색된 결과를 awk를 이용하여 = 구분자로 구분한후 2번째 컬럼을 출력
############################################################################
 
CDH_PARCEL_VERSION=`cat config.ini|grep ^cdh.parcel.version|awk -F\= '{print $2}'`
grep_version=`cat config.ini|grep cdh.parcel.version`
echo "grep_version" $grep_version
echo "CDH_PARCEL_VERSION" $CDH_PARCEL_VERSION
  
  
#############################################################
# 설치가능한 rpm 목록 출력을 위한 yum 명령어
#############################################################
예제
for rpm in `yum list available --disablerepo=* --enablerepo="soe-bigdata-c5" | awk -F\. '{print $1}'`; do
  f=`echo $rpm | awk -F '.' '{print $1}'`
  echo $f
  echo "############# RPM $f locations ##############" >> rpm_log.txt
done

#############################################################
# awk를 이용하여 csv의 평균값 구하기
#############################################################
awk -F, '{sum += $3} END{print sum / NR}' score.txt

#############################################################
# awk를 이용하여 로그파일의 컬럼 출력순서 변경하기
#############################################################
awk '{prnt $3, $2, $1} score.txt

  
########################################
# how to use sed option
below text is test.txt
one two three, one two three
four three two one
one hundred
and you used the command
########################################
1 단순 찾아 바꾸기
   sed 's/ovr-usr/ovr-user/' df.txt > df-Ph.txt
   아래 명령어는 위와 동일 하다.
   sed 's/ovr-usr/ovr-user/' <df.txt > df-Ph.txt
   
2. 파일에서 찾아 바꾸기
   g(global) 옵션이 없으면 한줄에서 첫번째 패턴 매치되는 단어만 replace된다.
   g(global) 옵션이 있으면 전체파일에서 패턴 매치되는 모든 단어만 replace된다.
   sed 's/one/ONE/g' < test.txt   
   sed 's/one/ONE/' < test.txt   

   output :
   ONE two three, one two three
   four three two ONE
   ONE hundred
   and you used the command

   
   
3 escape 문자의 처리 원본은 놔두고 처리된 새파일로 저장하기
  sed s/day/night/ old.txt >new.txt
   
4 escape 문자의 처리   
   echo "/usr/local/bin" | sed 's/\/usr\/local\/bin/\/common\/bin/' 
   output :
   /common/bin/
   
5. escape 문자의 처리없이 특수문자 처리(:)
   (|, _ 도 동일한 기능을 한다.)
   echo "/usr/local/bin/" | sed 's:/usr/local/bin/:/common/bin/:' 
   output :
   /common/bin/
   
6. 첫번째 줄 다음에 append 
   sed '1a many number' test.txt
   output :
   one two three, one two three
   many number
   four three two one
   one hundred
   and you used the command

7. 첫번째 줄에 insert
   sed '1i many number' test.txt
   output :
   many number
   one two three, one two three
   four three two one
   one hundred
   and you used the command
   
8. 행이 포함된 라인 번호 찾기
   sed -n '/hundred/=' test.txt
   output :
   3
   
9. 전체 라인 수를 리턴한다.
   sed -n '$=' test.txt
   output :
   4
   
10. 마지막 line을 replace 한다.
   sed '$c last line of the file' test.txt 
   output :
   one two three, one two three
   four three two one
   one hundred
   last line of the file
   
11. 첫번째 line을 replace 한다.   
   sed '1c first line of the file' test.txt 
   
12. 패턴 검색하여 찾은열 다음열에 문자열을 삽입한다.
        kdc_timesync = 0
        ticket_lifetime = 8h
        renew_lifetime = 24h
        kdc_timeout = 3000
        max_retries = 1


   sed '/kdc_timesync = 0/a         ticket_lifetime = 8h\n        renew_lifetime = 24h\n        kdc_timeout = 3000' test.txt   
   output :
   one two three, one two three
   four three two one
   fourth
   one hundred
   and you used the command
   
13. 패턴 검색하여 찾은열 전열에 문자열을 삽입한다.
   sed '/four/i fourth' test.txt   
   output :
   one two three, one two three
   four three two one
   fourth
   one hundred
   and you used the command   
   

14. 패턴 찾아서 찾은패턴을 복제하기.  
  echo "123 abc" | sed 's/[0-9]*/& &/'   

15. 대소문자를 무시하고 패턴 찾아서 복사하기
  sed -r 's/GONG/kang/I' file.txt

16. 패턴 검색하여 찾은열을 삭제한다.
  sed ‘/src/d’ fair3.txt
  
17. 3째 줄을 삭제한다.
  sed ‘3d’ fair3.txt
   
   
########################################
# 숫자 비교
# gt lt le ge eq ne(not equal)
########################################   
num1=$1
num2=$2
num3=$3

echo "param1 $num1"
echo "param2 $num2"
echo "param3 $num3"

if [ $num1 -eq $num2 ]; then
  echo "param1 and parma2 same"
elif [ $num1 -gt $num2 ]; then
  echo "param1 is greater than parma2"
elif [ $num1 -lt $num2 ]; then
  echo "param1 lt less than parma2"
fi

if [ $num1 -ge $num2 ]; then
  echo "param1 is greater or same than parma2"
fi

if [ $num1 -le $num2 ]; then
  echo "param1 is less or same than parma2"
fi

####################################################
#and or expression
#gt, lt, eq, ge, le
####################################################
if [ $num1 -gt $num2 ] && [ $num1 -gt $num3 ]; then
  echo "param1 is gretest of all"
fi

if [ $num1 -gt $num2 ] || [ $num1 -gt $num3 ]; then
  echo "param1 is grater than param2 or param3"
fi

####################################################
# expr 쉘스크립트에서 계산하기
####################################################
 $> echo `expr 10 + 10`
    20
 $> echo `expr 10 - 10`
    0
 $> echo `expr 10 "*" 10`
    100
 $> echo `expr 10 / 10`
    1
	
####################################################
# expr을 이용한 숫자 비교
####################################################
a=5
expr $a = 5
결과값 1 
exit코드는 0을 리턴한다.
프로그래밍 언어에서는 1이 true고 0이 false이다.
하지만 unix 시스템에서 명령어 성공이면 0을 반환한다.

위의 결과값을 가지고 같은지 비교를 할 수도 있지만
아래와 같은 코드가 헤깔리지 않는다.
a=5
if [ $a -eq 5 ]; then
  echo "same"
fi


####################################################
# 숫자 기호를 사용한 표현
#use ((
####################################################

if  (( "$num1" == "$num2" )); then
  echo "use ((  :  param1 and parma2 are same"
elif (( "$num1" > "$num2" )); then
  echo "use ((  : param1 is greater than parma2"
elif (( "$num1" < "$num2" )); then
  echo "use ((  : param1 lt lower than parma2"
fi

##########################
# 숫자 연산 표현
#use ((
##########################

for i in {1..100}
do
  # 산술 확장을 이용해서 파일명에 3을 곱해서 계산한
  # 값을 텍스트 파일에 저장
  #(2)
  echo $((i * 3)) > ${i}.txt
done
########################################
# 조건에 만족하는 숫자 출력
# 숫자만 출력할 수 있다. 
# 문자는 출력하지 못함.
########################################
a=3
b=2
echo $(( a < b ? a : b ))

########################################
# 문자 비교, 문자 테스트
######################################## 

char1="$1"
char2="$2"

echo "param1 $char1"
echo "param2 $char2"

if [ $char1 == $char2 ]; then
  echo "== : param1 and param2 are same"
elif [ $char1 != $char2 ]; then
  echo "!= : param1 is not equal parma2"
fi

if [ ! -z "$char1" ]; then
  echo "! -z : param1 length is not zero"
else
  echo "! -z : param1 length is zero"
fi  
if [ -n "$char1" ]; then
  echo "!= : param1 is not null"
else 
  echo "!= : param1 is null" 
fi

########################################
# 문자 연결
######################################## 

today=`date  +%Y%m%d`
echo ${today}_log.log

############################################################################
# 배열의 선언과 추출
# { 를 붙이면 one을 추출하지만
# { 를 붙이지 않으면 첫번재 요소 zero에 [1]를 연결한 문자를 리턴 한다.
# 아래 코드 실행 안됨.
############################################################################

declare -a number=("zero","one","two")
echo ${number[1]}

############################################################################
# 로그파일에서 "PluginManifestParser" 문자열 갯수 출력
# grep -c 
############################################################################
logfile="/var/app/datameer/current/logs/conductor.log"
filter_count=$(grep -c "PluginManifestParser" $logfile)
echo "ERROR count : $filter_count"

############################################################################
# 파일에서 줄 수 출력
# wc <파일명>
# 옵션 -c(bytes) -m(char) -l(line) -w(word)
# output 줄수 단어수 라인수 파일명을 리턴한다.
# 6 20 87 test.txt
############################################################################
logfile="/var/app/datameer/current/logs/conductor.log.1"
char_count=$(wc  $logfile)
echo "char count : $char_count"

############################################################################
# 날짜 관련 조작
############################################################################
#내일
date -d '1day' +%Y%m%d
#어제 
date -d '1day ago' +%Y%m%d
#2틀전
date -d '2day ago' +%Y%m%d
#다음달
date -d '1month' +%Y%m%d
#한달 전
date -d '1month ago' +%Y%m%d
#년도만 얻기
date +%Y
#월만 얻기
date +%m
#일만 얻기
date +%d
############################################################################
# 날짜관련 비교(epoch)
############################################################################
day1="20140102 10:10:10"
day2="20140101 10:10:10"
#epoch 초 얻기
#하루는 1440분 86400초
epoch_day1=$(date -d "$day1" '+%s')
epoch_day2=$(date -d "$day2" '+%s')
interval=`expr \( $epoch_day1 - $epoch_day2 \) / 86400`
echo $interval
## mysql unix time에서 시간 구하기 함수
select FROM_UNIXTIME(1333063912)

############################################################################
# 로그파일 작성시 날짜의 입력
############################################################################
LOG_FILE=LOG.txt
function log_it () {
    echo "$@"
    echo "[$(date +"%Y/%m/%d %T")]: $@" >> $LOG_FILE
}

log_it "This is Test!"


########################################
# 디렉토리의 파일리스트 표시
######################################## 
#!/bin/bash
for logfile in /tmp/*.log.*
do
  echo $logfile
done

###################################################### 
# basename 디렉토리의 경로를 제거한 파일리스트 표시
# basename에 2번째 파라메터로 확장자를 
# 입력하면 확장자가 제거된 파일리스트를 얻을수 있다.
###################################################### 
#!/bin/bash
for logfile in /tmp/*.log
do
  logname=$(basename $logfile .log)
  echo $logname
done

확장자 제거하는 방법 2
filename="test.log"
filenm=${filename%.*]
echo $filenm

###################################################### 
# dos2unix
###################################################### 
function dos2unix () {
sed -i 's/\r//g' *.sh
sed -i 's/^M$//g' *.sh
}



############################################################################ 
# 메일 발송하기
#mailer_util.sh 
# SYNTAX: bash mailer_util.sh <FROM> <TO> <SUBJECT> <ATTACHMENT>
# EXAMPLE: bash mailer_util.sh "$FROM" "$TO" "$SUBJECT" "$CSVFILE"
############################################################################ 
FROM=$1
TO=$2
SUBJECT=$3
boundary="ZZ_/afg6432dfgkl.94531q"
body=`cat "$4"`

declare -a attachments
attachments=( $5 )

get_mimetype(){
  # warning: assumes that the passed file exists
  file --mime-type "$1" | sed 's/.*: //' 
}



# Build headers
{

printf '%s\n' "From: $FROM
To: $TO
Subject: $SUBJECT
Mime-Version: 1.0
Content-Type: multipart/mixed; boundary=\"$boundary\"

--${boundary}
Content-Type: text/plain; charset=\"US-ASCII\"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

$body
"
 
# now loop over the attachments, guess the type
# and produce the corresponding part, encoded base64
for file in "${attachments[@]}"; do

  [ ! -f "$file" ] && echo "Warning: attachment $file not found, skipping" >&2 && continue

  mimetype=$(get_mimetype "$file") 
 
  printf '%s\n' "--${boundary}
Content-Type: $mimetype
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename=\"$file\"
"
 
  base64 "$file"
  echo
done
 
# print last boundary with closing --
printf '%s\n' "--${boundary}--"
 
} | sendmail -t -oi   

############################################################################ 
# 디폴트 게이트웨이 주소 알아내기
# 디폴트 게이트웨이 : 외부네트웍크의 출입구가 되는 기기
# 자신이 속한 네트웍 기기와 통신시에는 게이트웨이를 사용하지 않음
# 네트웍에 이상이 생기면 디폴트게이트웨이가 통신이 되는지부터 ping으로 확인
# -n 옵션은 현재 경로 테이블 내용을 호스트명이 아닌 ip주소로 표시
# ping 실행시 -c 1 을 지정해야 한번만 실행하고 종료한다.
############################################################################ 
gateway=route -n | awk '$1 == "0.0.0.0" {print $2}'
아래 명령어는 위의 명령어와 동일한 결과를 리턴한다.
netstat -nr | awk '$1 == "0.0.0.0" {print $2}'

ping -c 1 $gateway >/dev/null 2>$1

아래의 결과에서 마지막 줄의 2번째열을 가지고 온다.

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
xxx.xxx.xxx.xxx    0.0.0.0         255.255.255.0   U     0      0        0 bond0
xxx.xxx.xxx.xxx    0.0.0.0         255.255.255.0   U     0      0        0 eth7
xxx.xxx.xxx.xxx    0.0.0.0         255.255.0.0     U     1009   0        0 eth7
xxx.xxx.xxx.xxx    0.0.0.0         255.255.0.0     U     1014   0        0 bond0
0.0.0.0           xxx.xxx.xxx.xxx    0.0.0.0         UG    0      0        0 bond0

############################################################################ 
# 특정 호스트의 평균 ping 응답시간 취득하기
# $$는 쉘특수 변수로 proccess ID가 들어간다.
# ping.$$ 파일에서 맨 마지막 부분을 읽어서 평균을 구한다.
# wc -l을 를 이용하여 ping.$$ 줄수를 센다.
# awk -F\/ -v line_cnt="$line_cnt"
# 위의 명령을 이용하여 구분자를 /로 지정한다.
# awk 내부변수를 -v line_cnt="$line_cnt" 처럼 설정한다.
# 현재의 레코드가 line_cnt가 NR(현재 레코드)와 같을 때 5번째 컬럼을 print 한다.
# 또는 현재의 렉코드가 총레코드수와 동일할때 5번째 컬럽을 출력한다.(NR==FNR)
# NR  :현재레코드번호
# FNR :전체 레코드 번호
# NF  :마지막 컬럼(현재 레코드 필드 수)
############################################################################ 
ping -c 20 xxx.xxx.xxx.xxx > ping.$$

--- xxxxxxxxxxxxxxxxxxx ping statistics ---
20 packets transmitted, 20 received, 0% packet loss, time 19002ms
rtt min/avg/max/mdev = 0.116/0.145/0.161/0.018 ms
    
	line_cnt=$(wc -l ping.18619 | awk '{print $1}')
    avg=$(awk -F\/ -v line_cnt="$line_cnt" 'NR==line_cnt {print $5}' ping.18619)
아래의 결과는 위의 결과와 동일 하다.
	ping -c 20 bdicr101x01h2 | awk -F\/ '/rtt min/ {print $5}
	avg=$(awk -F\/ 'NR==FNR {print $5}' ping.18619)
    echo $avg

############################################################################ 
# csv 파일에서 특정컬럼값만 얻기
# -f 옵션을 이용해서 추출할 컬럼 번호를 지정한다.
# -d 옵션을 이용해서 구분자를 지정합니다.
############################################################################ 
while read line
  do
   id=$(echo $line | cut -f 1 -d ',')
done < test.csv

while IFS=, read id name score
   do
      echo $id
	  echo $name
	  echo $score
	  echo " "
   done < score.txt
   
############################################################################ 
# 목록에서 문자열 끝문자로 정렬하기
# list.txt 파일을 반전시킨다. > 소트한다 > 원래데로 반전한다.
############################################################################    
rev list.txt | sort | rev > temp.txt

############################################################################ 
# cut을 이용하여 환경설정에서 -d로 구분자를 지정 -f로 필드 number를 지정
############################################################################ 
   env | grep "PATH" | cut -f 2 -d "="

############################################################################ 
# shell에서 생성된 파일의 권한지정
# 퍼미션이 600인 파일을 생성한다.
############################################################################ 
  umask 077
  
############################################################################ 
# grep 사용방법
# -i 옵션 대소문자를 구문하지 않는다.
# -n 행번호를 함께 출력한다.
# -v 지정된 행과 일치하지 않는 행만 출력한다.
# -q 출력없이 있는지 없는지만 체크할때
# -c 패턴과 일치하는 횟수를 리턴한다.
# -E egrep과 동일 한 결과를 리턴한다.
############################################################################ 
"^kang" kang로 시작하는 문자열 검색
  grep "^kang" list.txt

"_y$" _y로 끝나는 문자열 검색
  grep "_y$" list.txt
  
"l..e" l로 시작하고 e로 끝나는 4자리 문자열 검색
  grep "l..e" list.txt
  
"l*e" l로 시작하고 e로 끝나는 자리수 구분없이 모든 문자열 검색
  grep "l*e" list.txt  
  
"^[sr]" s또는 r로 시작하는 줄을 검색한다.
  grep "^[sr]" list.txt  
  
"^[0-9]" 숫자로 시작하는 줄을 검색한다.
  grep "^[0-9]" list.txt 
  
"^[a-z]" 소문자로 시작하는 줄을 검색한다.
  grep "^[a-z]" list.txt  
"^[A-Z]" 대문자로 시작하는 줄을 검색한다.
"^[A-Z][A-Z]" 대문자가 연속으로 2개 나오는 줄을 검색한다.
"^[A-Z]\." 대문자다음에 .이 나오는 줄을 검색한다.
"[0-9]\{9\}" 숫자가 9자리 이상인 줄을 검색한다.
grep -E '(kang|lion)' list.txt
아래명령어는 위와 결과가 동일하다.
egrep "(kang|lion)" name.txt

############################################################################ 
# 히어 도큐먼트 사용방법
# 쉘스크립트 본체의 문자열을 그대로 출력을 할때 사용한다.
# 히어도큐먼트 종료문자열을 ''로 감싸면 body의 변수와 명령어는 치환되지 않는다.
# 히어도큐먼트 종료문자열에 '' 없으면 변수와 명령어는 치환된다.
# 종료문자열에 ''없이 변수를 치환하고 싶으면 escape하면 된다.
# \$here
# 명령어 << '종료문자열'
# 히어도큐먼트 body
# 종료문자열 
############################################################################ 

here="first"

cat << _EOT_
this is here doc body
$here this variable  resolve
\$here this variable not resolve
`echo 'abc'`
_EOT_
아래는 실행 결과 이다.
this is here doc body
first this variable  resolve
$here this variable not resolve
abc

ftp에서 사용 예제
변수를 확장 할것 이므로 ''를 사용하지 않음
server="xxx.xxx.xxx.xxx"
user="user1"
password="xxxxxxx"
dir=/tmp/test.log
filename="test.log"

ftp -n "$server" << _EOT_
user "$user1" "$password"
bin
cd "$dir"
get "$filename"
_EOT_


############################################################################ 
# 히어 스트링 사용법
# 히어도큐먼트와 동일하지만 <<< 를 사용한다.
# 변수를 치환하고 싶으면 않으면 ''를 사용한다.
############################################################################ 

var="hello"

cat <<< "here string :
$var World !
"
cat <<< 'here string:
$var World !
'
아래는 결과 이다.
here string :
hello World !

here string:
$var World !

dd if=/dev/zero of=test.dat count=1024 bs=1024 2>/dev/null

############################################################################ 
# time 명령어
# time 명령어는 지정한 명령어의 실행시간과 cup시간을 나타낸다.
# real은 명령어 시작에서 종료된 시간을 나타낸다.
# user는 사용자 cpu 시간을 나타낸다.
# sys는 시스템 cpu 시간을 나타낸다.
# user+sys가 cpu 시간이고 real - (user+sys)가 I/O 대기시간으로 볼 수 있다.
# -p 옵션이 없으면 단위없이 출력이 된다.
############################################################################ 

bash-4.1$ time -p tar cvfz test.tar.gz test.*
test.dat
test.err
test.out
real 0.01
user 0.00
sys 0.00






citi의 정보보호 단계

1. cluster 자체의 접근
   - 클러스터에 대한 access는 전용 프록시 서버를 거쳐야 함.
   - 모든 fid는 kdc/ldap에 생성된다.
     - 로컬 fid는 생성이 금지됨.
   - ldap에 호스트 그룹이 별도로 생성. 
     GPA / CATE, SA, DBA 용으로 만들어진 별도의 호스트 그룹생성. 
   - soeid에 대한 서버의 접근권한을 부여한다.
   - 어플리케이션 운영에 필요한 ldap id, group 생성
   - soeid에 어플리케이션 운영에 필요한 2차그룹을 부여한다.
   - 사용자별 사용명령어 제한
   - ScapDAP PBRUN access setup
	 - 사용자 별로 switching user 할수 있는 fid를 등록한다.(pbrun)
	 - switching user를 할때는 사전에 FID 사용신청을 통해 결재를 득해야 함.
   - soeid에 대해 실행할 수 있는 명령어를 부여하고 그 이외의 명령은 제한을 함.
   - Role에 따라 루트권한이 필요한 경우 권한상승을 통해 일부 명령어를 등록함.
     ex) service, chown 등등
   -ScapDap Powerbroker 정책은 Big Data Platform 클러스터에서 활동을 수행하기 위해 
    사용자가 각 FID로 pbrun하도록 설정됩니다.
    Functional ID에 pbrun을 수행 한 후 수행되는 활동은 
	내부감사 팀 및 관리자가 검토하기위한 Key Stroke 로그를 생성합니다.
	 
2. hdfs와 데이터 접근제한
   - 접근인 필요한 어플리케이션 FID의 커버로스 keytab 파일 및 pricipal 생성	
   - 인증 / 권한 부여는 KDC를 통해 설정되며 사용자는 Keytab을 이용하여 kinit을 실행하여 
	  토큰을 가져 와서 활동을 수행합니다.
   - hdfs 홈디렉토리 생성 
	  - soeid로긴시 본인에 홈디렉토리에만 접근이 가능하고
	  - 아래와 같은 형태로 생성한다.
	  - 퍼미션을 750 또는 770 으로 주고 그룹을 groupA로 부여합니다. groupA 프로젝트가 속한 그룹입니다.
	  - 이렇게 하면 groupA에 속한 다른 그룹 멤버도 파일을 읽을 수 있습니다.
	  - tmp 폴더는 스티키 비트를 부여야여 파일의 이동이나 삭제를 슈퍼유저와 소유자만 할 수있에 합니다.
	  
		$>hadoop fs -mkdir /user/user1
        $>hadoop fs –chown user1 /user/user1
        $>hadoop fs –chgrp groupA /user/user1 
        $>hadoop fs –chmod 750 /user/user1 
		
		$>hadoop fs -mkdir /data 
        $>hadoop fs –chown funcA /data/funcA
        $>hadoop fs –chgrp groupA /data/funcA 
        $>hadoop fs –chmod 770 /data/funcA 
		
		$>hdfs hadoop fs -chmod 1777 /tmp

 
   - 프로젝트 레벨의 데이터에 접근하기 위해서는 사전에 fid 사용 결재를 득한 후 소프트 토큰을 발생 후 접근가능
   - 커버로스 keytab 파일은 FID 별로 생성 FID로 스위칭 한경우 자신의 FID만 read가 가능(600)
	  - 즉 스위칭유저 할 수 있도록 사전에 정의 되어 있지 않으면 접근이 불가능 함.

3. 프로젝트 별 정보보호
   프로젝트 별로 landing 파일 또는 프로젝트별 고유 아키텍쳐별로 소프트웨어는
   아래와 같은 구조로 생성하고 <project name> 이하 디렉토리는 파일 퍼미션을 770으로 설정
    /data/1/<project name>/bin
	/data/1/<project name>/logs
	/data/1/<project name>/data
	/data/1/<project name>/tmp
	  
   네트웍 격리를 통한 시스템 보호

2. 클러스터 내부의 인프라
   ldap은 자격을 관리하는 데 사용됨
   커버로스는 인증을 하는데 사용됨
  
  - 모든 사용자는 하둡 클러스터의 서비스 fid 또는 데이터베이스 owner fid로 스위칭 할 수 있는 권한
    을 획득해야 접근인 가능하다
  - 모든 사용자는 Hadoop 인프라에 접근하기 위해서는 user 스위칭을 통해 키탭을 이용하여 
    커버로스 티켓이 있어야 한다.
  - 커버로스 티켓은 암호가 아닌 keytab을 통해 발급받을 수 있으며 퍼미션은 600으로 세팅이 되어 있어
    서비스 fid 또는 데이터베이스 owner fid 스위칭 하지 않고는 keytab에 접근을 할 수 없다.
  

3. sentry 및 그룹에 따른 hive 정보보호

	  id    : 
		 nextgenk : 데이터 베이스 owner fid
					위의 fid는 아래의 그룹을 모두 가짐
	  group : 
		 nextgenk        : 읽기 권한을 가지는 그룹
		 nextgenk_owner  : all 퍼미션을 가지는 그룹
		 
	  role : 그룹과 동일한 이름으로 role을 생성
		CREATE ROLE nextgenk
		GRANT ROLE nextgenk TO GROUP nextgenk
		CREATE ROLE nextgenk_owner
		GRANT ROLE nextgenk_owner TO GROUP nextgenk_owner	  
		GRANT ALL ON URI '<database URL>' TO ROLE nextgenk
		GRANT ALL ON URI '<database URL>' TO ROLE nextgenk_owner
		GRANT ALL ON URI 'file:///opt/cgfiles/common/jdbc' TO ROLE nextgenk
		GRANT ALL ON URI 'file:///opt/cgfiles/common/jdbc' TO ROLE nextgenk_owner
		GRANT select ON DATABASE l1_nextgenk TO ROLE nextgenk
		GRANT all ON DATABASE l1_nextgenk TO ROLE nextgenk_owner
		 
	  - 데이터 베이스에 접근을 하는 사용자의 접근 목적에 따라 사용자 ID에 그룹을 선택적 부여  
	    그룹을 더 세부항목으로 나눠서 테이블 레벨 또는 컬럼레벨로 사용자에 권한을 부여할 수 있다.
      - 모든 데이터베이스 테이블은 owner와 그룹은 hive 로 지정이 되고
        nextgenk 그룹은 데이터베이스 HDFS URL 하위의 모든 테이블 데이터에 r-x 권한을 가진다.
        nextgenk_owner 그룹은 데이터베이스 HDFS URL 하위의 모든 테이블 데이터에 대해 rwx 권한을 가진다.
	 
6. pii 데이터의 정보보호
   redaction
   masking
   
7. 레이어 분리를 통한 정보보호
   redaction
   masking   
 
8. 접근 감사
   navigator audit 테이블 및 로그 감시
	 
9. PIPA(Personal Information protection act)
   
	 
10. SAML
    - 보안 도메인간에 인증(authentication)과 권한부여(authorization)에 관련된 자료를 교환할 수 있는 xml 기반의 표준이다. 
	- 보안도메인은 IDP(identity provider) 와 sp(Service provider)로 구성되어 있다.
	- SAML의 대표적인 사용 사례가 SSO 이다.
	- IDP는 SSO 서버로 인증에 대한 부분과 권한부여를 담당한다.
	- IDP는 인증후에 권한부여 부분을 XML형식으로 Requester에게 리턴을 하게 된다.
	- SAML 구성요소는 principal (일반적으로 사용자), IDP 및 SP의 세 가지 역할을 정의된다. 
	- SAML에서 사용자 SP 에게 서비스를 요청합니다. SP는 IDP에 ID 신원을 요청하고 획득(XML)합니다. 
	  - 클라우데라 매니저 URL을 액세스 하면 SSO사이트로 REDIRECT 됨
	    - CITI에서 안되고 있음 바로 SSO 사이트로 접금
	      CM사이트로 접근하려고 하면 아래와 같이 에러가 발생함.
		  Authentication Failed: Error validating SAML message
	- 이 XML 파일을 토대로 SP는 액세스 제어 결정을 내릴 수 있습니다. 
	- 연결된 클라이언트에 대해 일부 서비스를 수행할지 여부를 결정할 수 있습니다.
	- 사용자 는 웹 브라우저를 통해 SAML SP로 보호되는 웹 리소스를 요청합니다. 
	- 요청하는 사용자의 신원을 알고 싶어하는 SP는 사용자 에이전트(saml plugin)를 통해 SAML IDP에 인증 요청을 보냅니다.
	- Cloudera Manager는 SP 및 IDP-initatied SSO를 모두 지원합니다.
    - Cloudera Manager의 로그 아웃 작업은 단일 로그 아웃 요청을 IDP로 보냅니다.
	
    SAML 세팅
	- CM은 SAML은 아래와 같은 절차로 Setting 되어집니다.
	  Peparing Files
      Configuring Cloudera Manager
      Configuring the IDP
      Verifying Authentication and Authorization

	
	####################################################################################
사전요구사항
	- auto_build_cluster.sh는 루트로 실행되어야 함.
	- passwordless SSH 계정 세팅이 되어 있어야 함.
	- 사전에 또는 설치중에 모든 소스가 지정된 폴더에 배포되어야 한다.
	- 루트 사용자의 비밀번호는 공백 (기본값)이고 포트 3306에서 실행되는 한 사용할 수 있습니다.
	- TLS 구성은이 빌드 된 프로세스에서 지원되지 않습니다.
	- MySQL 서버를 추가 / 구성 할 수 있지만 데이터베이스 간의 MySQL 복제는 구성되지 않습니다.
	- YUM repository를 사전에 설치 하는것이 좋습니다.
	- yum install BD_bdeng_scripts --disablerepo = * --enablerepo = soe-bigdata-c5
	  위와 같이 하면 autobuild관련 스크립트가 특정폴더에 생성됨.
	  
config.ini파일
    - passwordless SSH 설정된 계정에 대해 아래와 같이 정의해야 합니다.
    - ssh.private.key=/home/gpainstall/id_dsa_2048_a.pem
    - ssh.root.user=gpainstall
    - MASTER EDGE DATA 호스트를 지정을 할때 여러대인 경우는 ,로 구분을 하고 공백이 있으면
      않됩니다.
	- cluster.services는 설치될 서비스를 지정합니다. 
      cluster.services=FLUME,HIVE,HUE,IMPALA,OOZIE,SQOOP,SQOOP_CLIENT
	- os.mail_dl
	  CM의 Alert publisher가 사용할 메일 주소입니다.
	- java.rpm
	  설치할 java rpm 파일의 위치를 기술합니다.
	  java.rpm=/home/gpainstall/jdk-1.7.0_95l64-linux.x86_64.rpm
	- krb.enable=NO
	  클러스터를 빌드하는 동안 Kerberos 기반 인증 활성화하거나 사용하지 않는 옵션입니다. 
	  RELRM을 올바른 값으로 업데이트하고 모든 키탭이 /opt/Cloudera/keytabs 
	  디렉토리 (CM 서버에만 해당)에 모든 키탭이 존재해야 합니다.
	- krb.security.realm
	  커버로스 RELRM을 지정합니다.
	  krb.security.realm=EXEMDEV.EXEM.COM
prepare_host.py
    - 사전에 설치 auto_build_cluster.sh 내에서 호출되기도 하지만
	  사전에 아래와 같이 실행하여 사전준비사항(java설치 등)을 수행할 수 있다.
    ./prepare_host.py -p config.ini
auto_build_cluster.sh 
    - root 계정으로 아래의 파일을 아래와 같이 실행해야 합니다.
	- 설치 로그는 /var/log/CATE 디렉토리에 생성됩니다. 
	./auto_build_cluster.sh -p config.ini
cloudera manager login
    - 설치가 완료되면 아래의 url로 로그인을 합니다.
	- 기본계정은 admin:admin 입니다.
    - http://<your-vm-name>.nam.nsroot.net:7180/cmf/login   
os_setup.sh
    - 파라메터에 따라 설치를 하기도
      검증을 하기도 하는 쉘스크립트이다.
os_setup.sh –p config.ini –v
####################################################################################
1.auto_build_cluster.sh
========================
  auto_build_cluster.sh가 실행되는 서버와 환경설정의 cm.host를 비교하여
  같은면 환경설정의 버전에 해당하는 BD_cloudera_cdh_parcel을 수행한다.
  이때 repo 조회 명령을 수행하여 config.ini에 설정된 버전이 repo에 유휴한지 체크한다.
  아래의 경로에 manifest파일이 존재하는지 체크한다.
  아래 파일이 있어야 성공적으로 설치가 된것이다.
  /opt/cloudera/parcel-repo/CDH-5.7.1-1.cdh5.7.1.p1829.1855-el6.parcel.manifest
  있으면 config.ini파일에 아래처럼 update를 한다.
  parcel.manifest=/opt/cloudera/parcel-repo/CDH-5.7.0-1.cdh5.7.0.p0.45-el6.parcel.manifest
  기타 계정체크,서버갯수 체크등이 끝나면
  prepare_host.py 를 실행한다.
2. prepare_host.py
========================
  2-1: host in MYSQL_HOSTS:
  	  install_on_mysql() != 0:
  		pre_check
		  - yum repo가 있는지 체크하여 없으면 설치한다.
  		os_setup
		  - config.sh 실행
		  - config.ini MASTER PROXY DATA 노드가 제공되었는지 체크
		  - swappiness, ulimit, selinux 등을 세팅
  		install_java
		  - java 설치
  		install_bd_repo
		  - BD_bigdata_repo 설치
		  - prepare_host에서 설치
  		install_mysql
		  - install_mysql.sh 호츨
		  - DB 노드인경우 MYSQL Server를 설치함
  		install_cm_agent
		  - cm_agent를 설치 함
  		service cloudera-scm-agent start
  2-2: host in CM_HOSTS:
  	  install_on_cm(host)
  		pre_check
  		os_setup
  		install_bd_repo
  		install_mysql
		- DB 노드가 아닌 경우 MYSQL Client를 설치함
  		install_java
  		install_cm_agent
  		install_cm_server
  		service cloudera-scm-agent start
  		keytab 생성
		아래 환경설정을 읽어서 아래 디렉토리의 퍼미션과 파일 퍼미션을 변경함
		- krb.gen_keytab.script=/opt/Cloudera/bin/retrieve_credentials.sh
  		db.mgmt.properties 수정
  		install_api
  		service cloudera-scm-server start
  
  2-3: host in CLUSTER_MASTER_HOSTS:
  	  install_on_master() != 0:
  		pre_check
  		os_setup
  		install_java
  		install_bd_repo
  		install_mysql
  		install_cm_agent
  		service  cloudera scm-agent start
  2-4: host in CLUSTER_PROXY_HOSTS:
  	  install_on_proxy() != 0:
  		pre_check
  		os_setup
  		install_bd_repo
  		install_mysql
  		install_java
  		install_cm_agent
  		service  cloudera scm-agent start		 
  2-5: host in CLUSTER_DATA_HOSTS:
  	  install_on_data() != 0:
  		pre_check
  		os_setup
  		install_java
  		install_bd_repo
  		install_mysql
  		install_cm_agent
  		service  cloudera scm-agent start		 
  2-6: host in CLUSTER_ANALYTICAL_HOSTS:
  	  install_on_data() != 0:
  		pre_check
  		os_setup
  		install_java
  		install_bd_repo
  		install_mysql
  		install_cm_agent
  		service  cloudera scm-agent start
3. deploy_cluster.py
========================
####################################################################################
  
## SAN, CA 등 인증서정보 확인 
openssl x509 -in Certificate.cer.txt -noout -text


인증서의 변환
1. PKCS#12 에서 개인키 추출
openssl pkcs12 -in CertAndKey.pfx -nocerts -nodes -out private.key

2.PKCS#12 에서 인증서 추출
> openssl pkcs12 -in CertAndKey.pfx -nokeys -out server.crt

3. PEM -> PKCS#12 
> openssl pkcs12 -export -in server.crt -inkey private.key -out new-cert.p12 

4. DER => PEM
> openssl rsa -inform DER -outform PEM -in private.der -out private.pem

5. PEM => DER
> openssl rsa -inform PEM -outform DER -in private.pem -out private.der

6. JKS Key => PEM (중간파일이 openssl.pfx가 필요하다.)
> openssl pkcs12 -export -in all-certs.pem -inkey private.key -out openssl.pfx
> keytool -importkeystore -srckeystore openssl.pfx -srcstoretype PKCS12 -destkeystore keystore

7. JKS format 으로 변환한뒤에 cachin 추가하는 방법
keytool -importkeystore -srckeystore cacerts.jks -destkeystore keystore

8. public 키에 cachain 병합하는 방법
  cp public.key  all-certs.pem //여기에 퍼블릭 키
  cp CitiInternalCAChain_PROD.pem caroot.chain.pem
  cat caroot.chain.pem >> all-certs.pem //둘 다 합친 파일

//jdk에 설치되 default 인증서에 citi ca chain 추가 하기(신뢰할 수있는 서버리스트를 추가)
//BDR를 위해서는 DEV에서는 DEV_cacerts.jks(dev)와 cacerts.jks(prod)를 추가해줘야한다.
//BDR를 위해서는 UAT에서는 UAT_cacerts.jks(dev)와 cacerts.jks(prod)를 추가해줘야한다.
//그래야만 HandShakeException을 피할 수 있게된다.

cp /usr/java/latest/jre/lib/security/cacert /tmp/certs/cacert_orig
cp /tmp/certs/cacert_orig /tmp/certs/truststore

//prod cachin 추가
keytool -importkeystore -srckeystore cacerts.jks -destkeystore truststore
//uat cachin 추가
keytool -importkeystore -srckeystore UAT_cacerts.jks -destkeystore truststore
//dev cachin 추가
keytool -importkeystore -srckeystore DEV_cacerts.jks -destkeystore truststoreD

cp /usr/java/latest/jre/lib/security/cacert

cmp를 통해 받은 private 키의 패스워드는 cmp에서 특수문자와 숫자를 입력해서 패스워드를 지정해야해서 변경이 필요하다.
아래 명령어를 통해 변경이 가능하다.
PEM format :
openssl rsa -des3 -in private.pem -out private2.pem

pem format 패스워드 삭제
openssl rsa -in private.key -out server.key 

jks format
keytool -storepasswd -new changeit -keystore cacerts

citica.truststore : changeit
사이트에서 다운로드 받은 citi cachain :changeit
위의 인증서 이외의 모든 인증서는 hadoop

데이터미어 인증서
/var/app/datameer/current/etc/keystore.dm : storepwd
Service Provider Key Alias : 1
Service Provider Key Passphrase : hadoop





############################################################################################################################################
# File Name: jhs_ha.py
# Author: Ashish Tyagi
# Date created: April 7, 2017
# Date last modified: April 13, 2017
# Python Version: 2.6
# CM Python API Version: 14
# Description: Following script is meant to replicate HA functionality for Yarn Job History server and automatically move it to a new host 
# Change Log
# Change Number | Date MM-DD-YYYY  | Changed By        | Change Description
# Initial       | 04-13-2017       | Ashish Tyagi      | Initial code draft 
# Initial       | 04-17-2017       | Ashish Tyagi      | Added logging and email alerts 
# Initial       | 05-01-2017       | Ashish Tyagi      | Added secure password reading
#
############################################################################################################################################

import os
import logging
import subprocess
import smtplib
import logging.config
import ConfigParser
import time, datetime
from cm_api.api_client import ApiResource, ApiException
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText
from email.MIMEImage import MIMEImage

# Define constants 
ConfigFilePath = "/opt/cgfiles/common/bin/jhs_ha/application.conf"
ServiceType = "YARN"
RoleType = "JOBHISTORY"
JHSHostHealthCheckName = "JOBHISTORY_HOST_HEALTH"

# Read application.conf file
config = ConfigParser.SafeConfigParser()
config.read(ConfigFilePath)

# Populate required variables from configuration file
clouderaManagerHost = config.get("clouderaManager", "clouderaManagerHost")
clouderaManagerPort = config.getint("clouderaManager", "clouderaManagerPort")
clouderaManagerApiVersion = config.getint("clouderaManager", "clouderaManagerApiVersion")
clouderaManagerHTTPS = config.getboolean("clouderaManager", "clouderaManagerHTTPS")
clouderaManagerUserName = config.get("clouderaManager", "clouderaManagerUserName")
clusterDisplayName = config.get("clouderaManager", "clusterDisplayName")
sendAlertEmail = config.getboolean("alertingService", "sendAlertEmail")
sendSuccessEmail = config.getboolean("alertingService", "sendSuccessEmail")
sendFailureEmail = config.getboolean("alertingService", "sendFailureEmail")
alertEmailIcon = config.get("alertingService", "alertEmailIcon")
alertEmailUseTLS = config.getboolean("alertingService", "alertEmailUseTLS")
alertEmailFromAddress = config.get("alertingService", "alertEmailFromAddress")
alertEmailToAddress = config.get("alertingService", "alertEmailToAddress")
SMTPServerAddress = config.get("alertingService", "SMTPServerAddress")
SMTPServerPort = config.getint("alertingService", "SMTPServerPort")
JHSPrimaryHost = config.get("jobHistoryServer", "JHSPrimaryHost")
JHSFailoverHosts = config.get("jobHistoryServer", "JHSFailoverHosts").split(',')
JHSGoodHostTimeThreshold = config.getint("JHSFailover", "JHSGoodHostTimeThreshold")
JHSBadHostTimeThreshold = config.getint("JHSFailover", "JHSBadHostTimeThreshold")
JHSGoodHostRestartThreshold = config.getint("JHSFailover", "JHSGoodHostRestartThreshold")
JHSBadHostRestartThreshold = config.getint("JHSFailover", "JHSBadHostRestartThreshold")
monitoringTimeThreshold = config.getint("FailoverScript", "monitoringTimeThreshold")
commandTimeOut = config.getint("FailoverScript", "commandTimeOut")
maintenanceModeCheck = config.getboolean("FailoverScript", "maintenanceModeCheck")
loggingConfigFile = config.get("FailoverScript", "loggingConfigFile")
loggerName = config.get("FailoverScript", "loggerName")


# Get secure passwords for login 
clouderaManagerPasswordMode = config.get("clouderaManager", "clouderaManagerPasswordMode")
clouderaManagerPassword = "admin"
if clouderaManagerPasswordMode == "Command":
    clouderaManagerPasswordCommand = config.get("clouderaManager", "clouderaManagerPassword").split(' ')
    clouderaManagerPassword = subprocess.Popen(clouderaManagerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
elif clouderaManagerPasswordMode == "Plain":
    clouderaManagerPassword = config.get("clouderaManager", "clouderaManagerPassword")
SMTPServerAuthentication = config.getboolean("alertingService", "SMTPServerAuthentication")
if SMTPServerAuthentication:
    SMTPServerPasswordMode = config.get("alertingService", "SMTPServerPasswordMode")
    SMTPServerPassword = "admin"
    if SMTPServerPasswordMode == "Command":
        SMTPServerPasswordCommand = config.get("alertingService", "SMTPServerPassword").split(' ')
        SMTPServerPassword = subprocess.Popen(SMTPServerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
    elif SMTPServerPasswordMode == "Plain":
        SMTPServerPassword = config.get("alertingService", "SMTPServerPassword")

# Enable logging and get logger handle 
logging.config.fileConfig(loggingConfigFile)
logger = logging.getLogger(loggerName)


# Define global variables
api = None
cluster = None
yarnService = None
currentJHS = None
currentJHSHostHealth = False


# Sending email when JHS is moved
def SendMail(**kwargs):
    logger.info("Sending JHS " + kwargs['Mode'] +" email")
    try:
        msg = MIMEMultipart('related')
        msg['From'] = alertEmailFromAddress
        msg['Bcc'] = alertEmailToAddress
        msg['Subject'] = "Cluster: " + str(cluster.displayName) +" - Yarn Job History Server Move (HA)"
        
        if kwargs['Mode'] == 'Move':
            msgText = MIMEText(
                               '<p>' + \
                               'Moving Job History Server to a New Host: <br>' + \
                               '<b>Move Reason:</b><br>' + \
                               kwargs['moveReason'] + '<br>' +\
                               '<b>Cluster Name: '+ str(cluster.displayName)  +'</b><br>' + \
                               '<b>Current Job History Server Host: </b>' + kwargs['currentHost'] + '<br>' + \
                               '<b>New Job History Server Host: </b>' + kwargs['newHost'] + '<br><br>' + \
                               '</br>Possible "Config change: Restart required" alerts can be seen on different services.( <img src="cid:image1"> ) <br>' + \
                               'These alerts seen on CM UI have no impact on the cluster and can be safely ignored.<br>' + \
                               'If anyone happens to have done a purposeful configuration change and saved them, ' + \
                               'then a restart on that particular service or instance would be needed.<br><br>' + \
                               'Thanks,<br>Operations' + \
                               '</p>', \
                               'html')
            msg.attach(msgText)
            img = open(alertEmailIcon, 'rb')
            msgImage = MIMEImage(img.read())
            img.close()
            msgImage.add_header('Content-ID', '<image1>')
            msg.attach(msgImage)
            
        elif kwargs['Mode'] == 'Success':
            msg.attach(MIMEText("Succeeded in moving Job History Server to new host", 'plain'))
        elif kwargs['Mode'] == 'Failure':
            msgText = MIMEText('<p>' + \
                               'Failed to move Job History Server to new host <br><br>' + \
                               '<b>Failure Reason: </b>' + \
                               kwargs['failureReason'] + '<br>' + \
                               '<b>Error message: </b>' + \
                               kwargs['failureErrorMessage'] + '<br>' + \
                               '</p>','html')
            msg.attach(msgText)
                
        server = smtplib.SMTP(SMTPServerAddress, SMTPServerPort)
        if alertEmailUseTLS:
            server.starttls()
        if SMTPServerAuthentication:
            server.login(alertEmailFromAddress, SMTPServerPassword)
        text = msg.as_string()
        server.sendmail(alertEmailFromAddress, alertEmailToAddress, text)
        server.quit()
    except Exception as err:
        logger.debug("Error sending email: " + str(err)) 

# Refreshes state for global variables representing cluster, yarn service and JHS role
def refreshGlobals():
    logger.debug("Refreshing handles")
    global cluster
    global yarnService
    global currentJHS
    try:
        logger.debug("Getting cluster handle")
        # Get required cluster
        cluster = api.get_cluster(clusterDisplayName)
        # Get Yarn service
        logger.debug("Getting Yarn service handle")
        for service in cluster.get_all_services():
            if service.type == ServiceType:
                yarnService = service
                break
        # Get current Job History Server   
        logger.debug("Getting current JHS role handle")
        for serviceRole in yarnService.get_all_roles():
            if serviceRole.type == RoleType:
                currentJHS = serviceRole
                break
    except ApiException as err:
        logger.error("Failed to refresh handles > " + str(err))    
            
def monitorJHS():
    global currentJHSHostHealth
    logger.info("Starting monitoring")
    while True:
        logger.info("Starting next monitoring iteration")
        try:
            # Get latest state for cluster, yarn service and JHS role    
            refreshGlobals()
            SleepFlag = False
            logger.debug("Getting running command list for cluster and Yarn service")
            clusterCommands = cluster.get_commands()
            yarnCommands = yarnService.get_commands()
            
            # Check JHS host's health
            logger.debug("Checking current job history server host health")
            currentJHSHostHealth = False
            for check in currentJHS.healthChecks:
                if check['name'] == JHSHostHealthCheckName and check['summary'] == 'GOOD':
                    currentJHSHostHealth = True
                    break
            logger.debug("Current JHS host health: " + str(currentJHSHostHealth and "Good" or "Bad"))
            
            # Check if JHS is up and running normally
            logger.debug("Checking current JHS status")
            currentJHSHealth = False
            if "entityStatus" in currentJHS._ATTRIBUTES:
                if currentJHS.entityStatus == "GOOD_HEALTH":
                    currentJHSHealth = True
            else:
                logger.debug("EntityStatus not found using Health Summary instead")
                if currentJHS.healthSummary == "GOOD":
                    currentJHSHealth = True
            logger.debug("Current JHS status: " + str(currentJHSHealth and "Good" or "Bad"))
            
            
            # Check if cluster is in maintenance mode or stopped
            logger.debug("Checking if cluster or yarn service is in maintenance mode or there is any command running on them")
            if maintenanceModeCheck and cluster.maintenanceMode:
                SleepFlag = True
                logger.debug("Cluster in maintenance mode skipping further checks")
            # Check if cluster is stopped or is stopping
            elif cluster.entityStatus in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Cluster in currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(clusterCommands) != 0 and clusterCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on cluster skipping further checks")
            # Check if Yarn service is in maintenance mode or stopped
            elif maintenanceModeCheck and yarnService.maintenanceMode:
                SleepFlag = True
                logger.debug("Yarn service is in maintenance mode skipping further checks")
            # Check if yarn service is stopped or is stopping
            elif yarnService.serviceState in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Yarn service is currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(yarnCommands) != 0 and yarnCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on Yarn service skipping further checks")
            # Check last JHSGoodHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than JHSGoodHostRestartThreshold move the JHS. 
            elif currentJHSHostHealth:
                logger.debug("Current JHS host health is GOOD checking last " + \
                             str(JHSGoodHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(JHSGoodHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * JHSGoodHostTimeThreshold)) 
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " + RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
		logger.debug("query is " + query)
                timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
			logger.debug("numberOfRestarts= " + str(numberOfRestarts))
                if numberOfRestarts < JHSGoodHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating JHS move to new host")
            # Check last JHSBadHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than JHSBadHostRestartThreshold move the JHS.    
            else:
                logger.debug("Current JHS host health is BAD checking last " + \
                             str(JHSBadHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(JHSBadHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * JHSBadHostTimeThreshold))
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " + RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
                timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
                if numberOfRestarts < JHSBadHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating JHS move to new host")
                    
            # One of the above conditions is met and we need to move the JHS
            if not SleepFlag:
                break  
            
            # Sleep for monitoringTimeThreshold minute's and then again start monitoring 
            time.sleep(60 * monitoringTimeThreshold) 
                 
        except ApiException as err:
            logger.error("Error while monitoring retrying iteration after " + str(monitoringTimeThreshold) +  " min error: " + str(err))
            time.sleep(60 * monitoringTimeThreshold)

def moveJHS():
    logger.info("Starting JHS move to new host ")
    logger.info("Move reason " + str(currentJHSHostHealth \
                and "Good JHS host health but reached threshold unexpected exits" \
                or "Bad JHS host health and reached threshold unexpected exits"))    
    
    moveReason = str(currentJHSHostHealth \
                and "Job History Server host is in Good health but unexpected exits reached threshold" \
                or "Job History server host is in Bad health and unexpected exits reached threshold")
    
    # Get latest state for cluster, yarn service and JHS role    
    refreshGlobals()
    
    # Create a list of available hosts for new JHS
    logger.debug("Finding new host to move JHS to")
    newHosts = [JHSPrimaryHost] + JHSFailoverHosts
    JHSCurrentHost = api.get_host(currentJHS.hostRef.hostId).hostname.encode('utf-8')
    newHosts.remove(JHSCurrentHost)
    logger.debug("Possible hosts list: " + str(newHosts))
    
    # Check health for new hosts to find available hosts
    availableNewHosts = [] 
    for hostName in api.get_all_hosts():
        if hostName.hostname in newHosts:
            host = api.get_host(hostName.hostId)
            if host.healthSummary == "GOOD":
                availableNewHosts += [host]
    newJHSHost = None
    if availableNewHosts != []:
        logger.debug("Hosts found: " + str([host.hostname for host in availableNewHosts]))
        newJHSHost = availableNewHosts[0]
    
    # Send moving JHS email
    SendMail(Mode='Move', moveReason=moveReason, currentHost=JHSCurrentHost, newHost=(newJHSHost.hostname if newJHSHost != None else 'N/A'))
    
    if newJHSHost == None:
        logger.critical("No available host to relocate JHS")
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="No available host to relocate JHS" , failureErrorMessage="None")
        return 
      
    # Issue Stop command on current JHS role and wait for command to finish
    try:
        logger.info("Stopping current JHS role")
        stopCommand = yarnService.stop_roles(currentJHS.name)
        stopCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to stop current JHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to stop current JHS role" , failureErrorMessage=str(err))
        return 
    
    # Delete currentJHS role 
    try: 
        logger.info("Deleting current JHS role")
        yarnService.delete_role(currentJHS.name)
    except ApiException as err:
        logger.critical("Failed to delete current JHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to delete current JHS role" , failureErrorMessage=str(err))
        return
    
    # Create new JHS role on first host from available host list 
    newJHS = None
    try:
        if newJHSHost != None:
            logger.info("Moving JHS to new host: " + newJHSHost.hostname)
            newJHS = yarnService.create_role( yarnService.name + '-' + RoleType + '-' + str(int(time.time())), 
                                              RoleType, 
                                              newJHSHost.hostId)
    except ApiException as err:
        logger.critical("Failed to create new JHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to create new JHS role" , failureErrorMessage=str(err))
        return
    
    # Deploy new client configuration on cluster
    try:
        logger.info("Deploying new client configuration on cluster")
        newJHS.update_config({'unexpected_exits_thresholds':'{"warning":"never","critical":"never"}'})
        configDeployCommand = cluster.deploy_client_config()
        configDeployCommand.wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to deploy new configuration on cluster > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to deploy new configuration on cluster " , failureErrorMessage=str(err))
        return
    
    # Restart the new JHS role
    try:
        logger.info("Starting new JHS")
        restartCommand = yarnService.restart_roles(newJHS.name)
        restartCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to restart new Job History Server > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to restart new Job History Server" , failureErrorMessage=str(err))
        return
    
    logger.info("JHS Move to new server completed successfully")
    if sendSuccessEmail:
        SendMail(Mode="Success")
    
if __name__ == '__main__':
    # create a process ID file in /var/run/jhsha
    pid = os.getpid()
    op = open("/var/run/jhsha","w")
    op.write("%s" % pid)
    op.close()
    
    logger.info("Starting script")
    # Get Cloudera manager api handle
    api = ApiResource(clouderaManagerHost, clouderaManagerPort, clouderaManagerUserName, clouderaManagerPassword, clouderaManagerHTTPS, clouderaManagerApiVersion)
    logger.info("Got API instance > " + str(api))
    # Monitor current status of Job history server and move to new host if required
    while True:
        # Monitor JHS state based on provided rules 
        monitorJHS()
        # Move JHS to a new host 
        moveJHS()
    
    
############################################################################################################################################
# File Name: shs_ha.py
# Author: Ashish Tyagi
# Date created: April 18, 2017
# Date last modified: April 23, 2017
# Python Version: 2.6
# CM Python API Version: 14
# Description: Following script is meant to replicate HA functionality for Spark History Server and automatically move it to a new host 
# Change Log
# Change Number | Date MM-DD-YYYY  | Changed By        | Change Description
# Initial       | 04-18-2017       | Ashish Tyagi      | Initial code draft 
# Initial       | 04-17-2017       | Ashish Tyagi      | Added logging and email alerts
# Initial       | 05-01-2017       | Ashish Tyagi      | Added secure password reading
#
############################################################################################################################################

import os
import logging
import subprocess
import smtplib
import logging.config
import ConfigParser
import time, datetime
from cm_api.api_client import ApiResource, ApiException
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText
from email.MIMEImage import MIMEImage

# Define constants 
ConfigFilePath = "/opt/cgfiles/common/bin/shs_ha/application.conf"
ServiceType = "SPARK_ON_YARN"
RoleType = "SPARK_YARN_HISTORY_SERVER"
SHSHostHealthCheckName = "SPARK_ON_YARN_SPARK_YARN_HISTORY_SERVER_HOST_HEALTH"

# Read application.conf file
config = ConfigParser.SafeConfigParser()
config.read(ConfigFilePath)

# Populate required variables from configuration file
clouderaManagerHost = config.get("clouderaManager", "clouderaManagerHost")
clouderaManagerPort = config.getint("clouderaManager", "clouderaManagerPort")
clouderaManagerApiVersion = config.getint("clouderaManager", "clouderaManagerApiVersion")
clouderaManagerHTTPS = config.getboolean("clouderaManager", "clouderaManagerHTTPS")
clouderaManagerUserName = config.get("clouderaManager", "clouderaManagerUserName")
clusterDisplayName = config.get("clouderaManager", "clusterDisplayName")
sendAlertEmail = config.getboolean("alertingService", "sendAlertEmail")
sendSuccessEmail = config.getboolean("alertingService", "sendSuccessEmail")
sendFailureEmail = config.getboolean("alertingService", "sendFailureEmail")
alertEmailIcon = config.get("alertingService", "alertEmailIcon")
alertEmailUseTLS = config.getboolean("alertingService", "alertEmailUseTLS")
alertEmailFromAddress = config.get("alertingService", "alertEmailFromAddress")
alertEmailToAddress = config.get("alertingService", "alertEmailToAddress")
SMTPServerAddress = config.get("alertingService", "SMTPServerAddress")
SMTPServerPort = config.getint("alertingService", "SMTPServerPort")
SHSPrimaryHost = config.get("sparkHistoryServer", "SHSPrimaryHost")
SHSFailoverHosts = config.get("sparkHistoryServer", "SHSFailoverHosts").split(',')
SHSGoodHostTimeThreshold = config.getint("SHSFailover", "SHSGoodHostTimeThreshold")
SHSBadHostTimeThreshold = config.getint("SHSFailover", "SHSBadHostTimeThreshold")
SHSGoodHostRestartThreshold = config.getint("SHSFailover", "SHSGoodHostRestartThreshold")
SHSBadHostRestartThreshold = config.getint("SHSFailover", "SHSBadHostRestartThreshold")
monitoringTimeThreshold = config.getint("FailoverScript", "monitoringTimeThreshold")
commandTimeOut = config.getint("FailoverScript", "commandTimeOut")
maintenanceModeCheck = config.getboolean("FailoverScript", "maintenanceModeCheck")
loggingConfigFile = config.get("FailoverScript", "loggingConfigFile")
loggerName = config.get("FailoverScript", "loggerName")

# Get secure passwords for login 
clouderaManagerPasswordMode = config.get("clouderaManager", "clouderaManagerPasswordMode")
clouderaManagerPassword = "admin"
if clouderaManagerPasswordMode == "Command":
    clouderaManagerPasswordCommand = config.get("clouderaManager", "clouderaManagerPassword").split(' ')
    clouderaManagerPassword = subprocess.Popen(clouderaManagerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
elif clouderaManagerPasswordMode == "Plain":
    clouderaManagerPassword = config.get("clouderaManager", "clouderaManagerPassword")
SMTPServerAuthentication = config.getboolean("alertingService", "SMTPServerAuthentication")
if SMTPServerAuthentication:
    SMTPServerPasswordMode = config.get("alertingService", "SMTPServerPasswordMode")
    SMTPServerPassword = "admin"
    if SMTPServerPasswordMode == "Command":
        SMTPServerPasswordCommand = config.get("alertingService", "SMTPServerPassword").split(' ')
        SMTPServerPassword = subprocess.Popen(SMTPServerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
    elif SMTPServerPasswordMode == "Plain":
        SMTPServerPassword = config.get("alertingService", "SMTPServerPassword")

# Enable logging and get logger handle 
logging.config.fileConfig(loggingConfigFile)
logger = logging.getLogger(loggerName)


# Define global variables
api = None
cluster = None
sparkService = None
currentSHS = None
currentSHSHostHealth = False


# Sending email when SHS is moved
def SendMail(**kwargs):
    logger.info("Sending SHS " + kwargs['Mode'] +" email")
    try:
        msg = MIMEMultipart('related')
        msg['From'] = alertEmailFromAddress
        msg['Bcc'] = alertEmailToAddress
        msg['Subject'] = "Cluster: " + str(cluster.displayName) +" - Spark History Server Move (HA)"
        
        if kwargs['Mode'] == 'Move':
            msgText = MIMEText(
                               '<p>' + \
                               'Moving Spark History Server to a New Host: <br>' + \
                               '<b>Move Reason:</b><br>' + \
                               kwargs['moveReason'] + '<br>' +\
                               '<b>Cluster Name: '+ str(cluster.displayName)  +'</b><br>' + \
                               '<b>Current Spark History Server Host: </b>' + kwargs['currentHost'] + '<br>' + \
                               '<b>New Spark History Server Host: </b>' + kwargs['newHost'] + '<br><br>' + \
                               '</br>Possible "Config change: Restart required" alerts can be seen on different services.( <img src="cid:image1"> ) <br>' + \
                               'These alerts seen on CM UI have no impact on the cluster and can be safely ignored.<br>' + \
                               'If anyone happens to have done a purposeful configuration change and saved them, ' + \
                               'then a restart on that particular service or instance would be needed.<br><br>' + \
                               'Thanks,<br>Operations' + \
                               '</p>', \
                               'html')
            msg.attach(msgText)
            img = open(alertEmailIcon, 'rb')
            msgImage = MIMEImage(img.read())
            img.close()
            msgImage.add_header('Content-ID', '<image1>')
            msg.attach(msgImage)
            
        elif kwargs['Mode'] == 'Success':
            msg.attach(MIMEText("Succeeded in moving Spark History Server to new host", 'plain'))
        elif kwargs['Mode'] == 'Failure':
            msgText = MIMEText('<p>' + \
                               'Failed to move Spark History Server to new host <br><br>' + \
                               '<b>Failure Reason: </b>' + \
                               kwargs['failureReason'] + '<br>' + \
                               '<b>Error message: </b>' + \
                               kwargs['failureErrorMessage'] + '<br>' + \
                               '</p>','html')
            msg.attach(msgText)
                
        server = smtplib.SMTP(SMTPServerAddress, SMTPServerPort)
        if alertEmailUseTLS:
            server.starttls()
        if SMTPServerAuthentication:
            server.login(alertEmailFromAddress, SMTPServerPassword)
        text = msg.as_string()
        server.sendmail(alertEmailFromAddress, alertEmailToAddress, text)
        server.quit()
    except Exception as err:
        logger.debug("Error sending email: " + str(err)) 


# Refreshes state for global variables representing cluster, spark service and SHS role
def refreshGlobals():
    logger.debug("Refreshing handles")
    global cluster
    global sparkService
    global currentSHS
    try:
        logger.debug("Getting cluster handle")
        # Get required cluster
        cluster = api.get_cluster(clusterDisplayName)
        # Get spark service
        logger.debug("Getting spark service handle")
        for service in cluster.get_all_services():
            if service.type == ServiceType:
                sparkService = service
                break
        # Get current Spark History Server   
        logger.debug("Getting current SHS role handle")
        for serviceRole in sparkService.get_all_roles():
            if serviceRole.type == RoleType:
                currentSHS = serviceRole
                break
    except ApiException as err:
        logger.error("Failed to refresh handles > " + str(err))    
            
def monitorSHS():
    global currentSHSHostHealth
    logger.info("Starting monitoring")
    while True:
        logger.info("Starting next monitoring iteration")
        try:
            # Get latest state for cluster, spark service and SHS role    
            refreshGlobals()
            SleepFlag = False
            logger.debug("Getting running command list for cluster and spark service")
            clusterCommands = cluster.get_commands()
            sparkCommands = sparkService.get_commands()
            
            # Check SHS host's health
            logger.debug("Checking current Spark History Server host health")
            currentSHSHostHealth = False
            for check in currentSHS.healthChecks:
                if check['name'] == SHSHostHealthCheckName and check['summary'] == 'GOOD':
                    currentSHSHostHealth = True
                    break
            logger.debug("Current SHS host health: " + str(currentSHSHostHealth and "Good" or "Bad"))
            
            # Check if SHS is up and running normally
            logger.debug("Checking current SHS status")
            currentSHSHealth = False
            if "entityStatus" in currentSHS._ATTRIBUTES:
                if currentSHS.entityStatus == "GOOD_HEALTH":
                    currentSHSHealth = True
            else:
                logger.debug("EntityStatus not found using Health Summary instead")
                if currentSHS.healthSummary == "GOOD":
                    currentSHSHealth = True
            logger.debug("Current SHS status: " + str(currentSHSHealth and "Good" or "Bad"))
            
            
            # Check if cluster is in maintenance mode or stopped
            logger.debug("Checking if cluster or spark service is in maintenance mode or there are any commands running on them")
            if maintenanceModeCheck and cluster.maintenanceMode:
                SleepFlag = True
                logger.debug("Cluster in maintenance mode skipping further checks")
            # Check if cluster is stopped or is stopping
            elif cluster.entityStatus in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Cluster in currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(clusterCommands) != 0 and clusterCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on cluster skipping further checks")
            # Check if spark service is in maintenance mode or stopped
            elif maintenanceModeCheck and sparkService.maintenanceMode:
                SleepFlag = True
                logger.debug("Spark service is in maintenance mode skipping further checks")
            # Check if spark service is stopped or is stopping
            elif sparkService.serviceState in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Spark service is currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(sparkCommands) != 0 and sparkCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on spark service skipping further checks")
            # Check last SHSGoodHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than SHSGoodHostRestartThreshold move the SHS. 
            elif currentSHSHostHealth:
                logger.debug("Current SHS host health is GOOD checking last " + \
                             str(SHSGoodHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(SHSGoodHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * SHSGoodHostTimeThreshold)) 
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " +  RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
                logger.debug("query is " + query)
		timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
			logger.debug("numberOfRestarts= " + str(numberOfRestarts))
                if numberOfRestarts < SHSGoodHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating SHS move to new host")
            # Check last SHSBadHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than SHSBadHostRestartThreshold move the SHS.    
            else:
                logger.debug("Current SHS host health is BAD checking last " + \
                             str(SHSBadHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(SHSBadHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * SHSBadHostTimeThreshold))
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " +  RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
                timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
                if numberOfRestarts < SHSBadHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating SHS move to new host")
                    
            # One of the above conditions is met and we need to move the SHS
            if not SleepFlag:
                break  
            
            # Sleep for monitoringTimeThreshold minute's and then again start monitoring 
            time.sleep(60 * monitoringTimeThreshold) 
                 
        except ApiException as err:
            logger.error("Error while monitoring retrying iteration after " + str(monitoringTimeThreshold) +  " min error: " + str(err))
            time.sleep(60 * monitoringTimeThreshold)

def moveSHS():
    logger.info("Starting SHS move to new host ")
    logger.info("Move reason " + str(currentSHSHostHealth \
                and "Good SHS Host health but reached threshold unexpected exits" \
                or "Bad SHS Host health and reached threshold unexpected exits"))
    
    moveReason = str(currentSHSHostHealth \
                and "Spark History server host is in Good health but unexpected exits reached threshold" \
                or "Spark History server host is in Bad health and unexpected exits reached threshold")  
    
    # Get latest state for cluster, spark service and SHS role    
    refreshGlobals()

    # Create a list of available hosts for new SHS
    logger.debug("Finding new host to move SHS to")
    newHosts = [SHSPrimaryHost] + SHSFailoverHosts
    SHSCurrentHost = api.get_host(currentSHS.hostRef.hostId).hostname.encode('utf-8')
    newHosts.remove(SHSCurrentHost)
    logger.debug("Possible hosts list: " + str(newHosts))
    
    # Check health for new hosts to find available hosts
    availableNewHosts = [] 
    for hostName in api.get_all_hosts():
        if hostName.hostname in newHosts:
            host = api.get_host(hostName.hostId)
            if host.healthSummary == "GOOD":
                availableNewHosts += [host]
    newSHSHost = None
    if availableNewHosts != []:
        logger.debug("Hosts found: " + str([host.hostname for host in availableNewHosts]))
        newSHSHost = availableNewHosts[0]
    
    # Send moving SHS email
    SendMail(Mode='Move', moveReason=moveReason, currentHost=SHSCurrentHost, newHost=(newSHSHost.hostname if newSHSHost != None else 'N/A'))
     
    if newSHSHost == None:
        logger.critical("No available host to relocate SHS")
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="No available host to relocate SHS" , failureErrorMessage="None")
        return 
           
    # Issue Stop command on current SHS role and wait for command to finish
    try:
        logger.info("Stopping current SHS role")
        stopCommand = sparkService.stop_roles(currentSHS.name)
        stopCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to stop current SHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to stop current SHS role" , failureErrorMessage=str(err))
        return 
    
    # Delete currentSHS role 
    try: 
        logger.info("Deleting current SHS role")
        sparkService.delete_role(currentSHS.name)
    except ApiException as err:
        logger.critical("Failed to delete current SHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to delete current SHS role" , failureErrorMessage=str(err))
        return
    
    # Create new SHS role on first host from available host list 
    newSHS = None
    try:
        if newSHSHost != None:
            logger.info("Moving SHS to new host: " + newSHSHost.hostname)
            newSHS = sparkService.create_role( sparkService.name + '-' + RoleType + '-' + str(int(time.time())), 
                                              RoleType, 
                                              newSHSHost.hostId)
    except ApiException as err:
        logger.critical("Failed to create new SHS role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to create new SHS role " , failureErrorMessage=str(err))
        return
    
    # Deploy new client configuration on cluster
    try:
        logger.info("Deploying new client configuration on cluster")
        newSHS.update_config({'unexpected_exits_thresholds':'{"warning":"never","critical":"never"}'})
        configDeployCommand = cluster.deploy_client_config()
        configDeployCommand.wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to deploy new configuration on cluster > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to deploy new configuration on cluster" , failureErrorMessage=str(err))
        return
    
    # Restart the new SHS role
    try:
        logger.info("Starting new SHS")
        restartCommand = sparkService.restart_roles(newSHS.name)
        restartCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to restart new Spark History Server > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to restart new Spark History Server" , failureErrorMessage=str(err))
        return
    
    logger.info("SHS Move to new server completed successfully")
    if sendSuccessEmail:
        SendMail(Mode="Success")
    
if __name__ == '__main__':
    # create a process ID file in /var/run/shsha
    pid = os.getpid()
    op = open("/var/run/shsha","w")
    op.write("%s" % pid)
    op.close()
    
    logger.info("Starting script")
    # Get Cloudera manager api handle
    api = ApiResource(clouderaManagerHost, clouderaManagerPort, clouderaManagerUserName, clouderaManagerPassword, clouderaManagerHTTPS, clouderaManagerApiVersion)
    logger.info("Got API instance > " + str(api))
    # Monitor current status of Spark History Server and move to new host if required
    while True:
        # Monitor SHS state based on provided rules 
        monitorSHS()
        # Move SHS to a new host 
        moveSHS()
    
    
############################################################################################################################################
# File Name: shs_ha.py
# Author: Ashish Tyagi
# Date created: April 18, 2017
# Date last modified: April 23, 2017
# Python Version: 2.6
# CM Python API Version: 14
# Description: Following script is meant to replicate HA functionality for Spark History Server and automatically move it to a new host 
# Change Log
# Change Number | Date MM-DD-YYYY  | Changed By        | Change Description
# Initial       | 04-18-2017       | Ashish Tyagi      | Initial code draft 
# Initial       | 04-17-2017       | Ashish Tyagi      | Added logging and email alerts
# Initial       | 05-01-2017       | Ashish Tyagi      | Added secure password reading
#
############################################################################################################################################

import os
import logging
import subprocess
import smtplib
import logging.config
import ConfigParser
import time, datetime
from cm_api.api_client import ApiResource, ApiException
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText
from email.MIMEImage import MIMEImage

# Define constants 
ConfigFilePath = "/opt/cgfiles/common/bin/shs2_ha/application.conf"
ServiceType = "SPARK2_ON_YARN"
RoleType = "SPARK2_YARN_HISTORY_SERVER"
SHSHostHealthCheckName = "SPARK2_ON_YARN_SPARK2_YARN_HISTORY_SERVER_HOST_HEALTH"

# Read application.conf file
config = ConfigParser.SafeConfigParser()
config.read(ConfigFilePath)

# Populate required variables from configuration file
clouderaManagerHost = config.get("clouderaManager", "clouderaManagerHost")
clouderaManagerPort = config.getint("clouderaManager", "clouderaManagerPort")
clouderaManagerApiVersion = config.getint("clouderaManager", "clouderaManagerApiVersion")
clouderaManagerHTTPS = config.getboolean("clouderaManager", "clouderaManagerHTTPS")
clouderaManagerUserName = config.get("clouderaManager", "clouderaManagerUserName")
clusterDisplayName = config.get("clouderaManager", "clusterDisplayName")
sendAlertEmail = config.getboolean("alertingService", "sendAlertEmail")
sendSuccessEmail = config.getboolean("alertingService", "sendSuccessEmail")
sendFailureEmail = config.getboolean("alertingService", "sendFailureEmail")
alertEmailIcon = config.get("alertingService", "alertEmailIcon")
alertEmailUseTLS = config.getboolean("alertingService", "alertEmailUseTLS")
alertEmailFromAddress = config.get("alertingService", "alertEmailFromAddress")
alertEmailToAddress = config.get("alertingService", "alertEmailToAddress")
SMTPServerAddress = config.get("alertingService", "SMTPServerAddress")
SMTPServerPort = config.getint("alertingService", "SMTPServerPort")
SHSPrimaryHost = config.get("sparkHistoryServer", "SHSPrimaryHost")
SHSFailoverHosts = config.get("sparkHistoryServer", "SHSFailoverHosts").split(',')
SHSGoodHostTimeThreshold = config.getint("SHSFailover", "SHSGoodHostTimeThreshold")
SHSBadHostTimeThreshold = config.getint("SHSFailover", "SHSBadHostTimeThreshold")
SHSGoodHostRestartThreshold = config.getint("SHSFailover", "SHSGoodHostRestartThreshold")
SHSBadHostRestartThreshold = config.getint("SHSFailover", "SHSBadHostRestartThreshold")
monitoringTimeThreshold = config.getint("FailoverScript", "monitoringTimeThreshold")
commandTimeOut = config.getint("FailoverScript", "commandTimeOut")
maintenanceModeCheck = config.getboolean("FailoverScript", "maintenanceModeCheck")
loggingConfigFile = config.get("FailoverScript", "loggingConfigFile")
loggerName = config.get("FailoverScript", "loggerName")

# Get secure passwords for login 
clouderaManagerPasswordMode = config.get("clouderaManager", "clouderaManagerPasswordMode")
clouderaManagerPassword = "admin"
if clouderaManagerPasswordMode == "Command":
    clouderaManagerPasswordCommand = config.get("clouderaManager", "clouderaManagerPassword").split(' ')
    clouderaManagerPassword = subprocess.Popen(clouderaManagerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
elif clouderaManagerPasswordMode == "Plain":
    clouderaManagerPassword = config.get("clouderaManager", "clouderaManagerPassword")
SMTPServerAuthentication = config.getboolean("alertingService", "SMTPServerAuthentication")
if SMTPServerAuthentication:
    SMTPServerPasswordMode = config.get("alertingService", "SMTPServerPasswordMode")
    SMTPServerPassword = "admin"
    if SMTPServerPasswordMode == "Command":
        SMTPServerPasswordCommand = config.get("alertingService", "SMTPServerPassword").split(' ')
        SMTPServerPassword = subprocess.Popen(SMTPServerPasswordCommand, stdout=subprocess.PIPE).stdout.read().strip()
    elif SMTPServerPasswordMode == "Plain":
        SMTPServerPassword = config.get("alertingService", "SMTPServerPassword")

# Enable logging and get logger handle 
logging.config.fileConfig(loggingConfigFile)
logger = logging.getLogger(loggerName)


# Define global variables
api = None
cluster = None
sparkService = None
currentSHS = None
currentSHSHostHealth = False

# Sending email when SHS is moved
def SendMail(**kwargs):
    logger.info("Sending SHS2 " + kwargs['Mode'] +" email")
    try:
        msg = MIMEMultipart('related')
        msg['From'] = alertEmailFromAddress
        msg['Bcc'] = alertEmailToAddress
        msg['Subject'] = "Cluster: " + str(cluster.displayName) +" - Spark2 History Server Move (HA)"
        
        if kwargs['Mode'] == 'Move':
            msgText = MIMEText(
                               '<p>' + \
                               'Moving Spark2 History Server to a New Host: <br>' + \
                               '<b>Move Reason:</b><br>' + \
                               kwargs['moveReason'] + '<br>' +\
                               '<b>Cluster Name: '+ str(cluster.displayName)  +'</b><br>' + \
                               '<b>Current Spark2 History Server Host: </b>' + kwargs['currentHost'] + '<br>' + \
                               '<b>New Spark2 History Server Host: </b>' + kwargs['newHost'] + '<br><br>' + \
                               '</br>Possible "Config change: Restart required" alerts can be seen on different services.( <img src="cid:image1"> ) <br>' + \
                               'These alerts seen on CM UI have no impact on the cluster and can be safely ignored.<br>' + \
                               'If anyone happens to have done a purposeful configuration change and saved them, ' + \
                               'then a restart on that particular service or instance would be needed.<br><br>' + \
                               'Thanks,<br>Operations' + \
                               '</p>', \
                               'html')
            msg.attach(msgText)
            img = open(alertEmailIcon, 'rb')
            msgImage = MIMEImage(img.read())
            img.close()
            msgImage.add_header('Content-ID', '<image1>')
            msg.attach(msgImage)
            
        elif kwargs['Mode'] == 'Success':
            msg.attach(MIMEText("Succeeded in moving Spark2 History Server to new host", 'plain'))
        elif kwargs['Mode'] == 'Failure':
            msgText = MIMEText('<p>' + \
                               'Failed to move Spark2 History Server to new host <br><br>' + \
                               '<b>Failure Reason: </b>' + \
                               kwargs['failureReason'] + '<br>' + \
                               '<b>Error message: </b>' + \
                               kwargs['failureErrorMessage'] + '<br>' + \
                               '</p>','html')
            msg.attach(msgText)
                
        server = smtplib.SMTP(SMTPServerAddress, SMTPServerPort)
        if alertEmailUseTLS:
            server.starttls()
        if SMTPServerAuthentication:
            server.login(alertEmailFromAddress, SMTPServerPassword)
        text = msg.as_string()
        server.sendmail(alertEmailFromAddress, alertEmailToAddress, text)
        server.quit()
    except Exception as err:
        logger.debug("Error sending email: " + str(err)) 


# Refreshes state for global variables representing cluster, spark service and SHS role
def refreshGlobals():
    logger.debug("Refreshing handles")
    global cluster
    global sparkService
    global currentSHS
    try:
        logger.debug("Getting cluster handle")
        # Get required cluster
        cluster = api.get_cluster(clusterDisplayName)
        # Get spark service
        logger.debug("Getting spark service handle")
        for service in cluster.get_all_services():
            if service.type == ServiceType:
                sparkService = service
                break
        # Get current Spark History Server   
        logger.debug("Getting current SHS role handle")
        for serviceRole in sparkService.get_all_roles():
            if serviceRole.type == RoleType:
                currentSHS = serviceRole
                break
    except ApiException as err:
        logger.error("Failed to refresh handles > " + str(err))    
            
def monitorSHS():
    global currentSHSHostHealth
    logger.info("Starting monitoring")
    while True:
        logger.info("Starting next monitoring iteration")
        try:
            # Get latest state for cluster, spark service and SHS role    
            refreshGlobals()
            SleepFlag = False
            logger.debug("Getting running command list for cluster and spark2 service")
            clusterCommands = cluster.get_commands()
            sparkCommands = sparkService.get_commands()
            
            # Check SHS host's health
            logger.debug("Checking current Spark History Server2 host health")
            currentSHSHostHealth = False
            for check in currentSHS.healthChecks:
                if check['name'] == SHSHostHealthCheckName and check['summary'] == 'GOOD':
                    currentSHSHostHealth = True
                    break
            logger.debug("Current SHS2 host health: " + str(currentSHSHostHealth and "Good" or "Bad"))
            
            # Check if SHS is up and running normally
            logger.debug("Checking current SHS2 status")
            currentSHSHealth = False
            if "entityStatus" in currentSHS._ATTRIBUTES:
                if currentSHS.entityStatus == "GOOD_HEALTH":
                    currentSHSHealth = True
            else:
                logger.debug("EntityStatus not found using Health Summary instead")
                if currentSHS.healthSummary == "GOOD":
                    currentSHSHealth = True
            logger.debug("Current SHS status: " + str(currentSHSHealth and "Good" or "Bad"))
            
            
            # Check if cluster is in maintenance mode or stopped
            logger.debug("Checking if cluster or spark2 service is in maintenance mode or there are any commands running on them")
            if maintenanceModeCheck and cluster.maintenanceMode:
                SleepFlag = True
                logger.debug("Cluster in maintenance mode skipping further checks")
            # Check if cluster is stopped or is stopping
            elif cluster.entityStatus in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Cluster in currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(clusterCommands) != 0 and clusterCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on cluster skipping further checks")
            # Check if spark service is in maintenance mode or stopped
            elif maintenanceModeCheck and sparkService.maintenanceMode:
                SleepFlag = True
                logger.debug("Spark2 service is in maintenance mode skipping further checks")
            # Check if spark service is stopped or is stopping
            elif sparkService.serviceState in ["STOPPING", "STOPPED"]:
                SleepFlag = True
                logger.debug("Spark2 service is currently stopped skipping further checks")
            # Check if cluster is running any command right now
            elif len(sparkCommands) != 0 and sparkCommands[0].name in ["Start","Restart","Stop"]:
                SleepFlag = True
                logger.debug("There are running commands on spark2 service skipping further checks")
            # Check last SHSGoodHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than SHSGoodHostRestartThreshold move the SHS. 
            elif currentSHSHostHealth:
                logger.debug("Current SHS2 host health is GOOD checking last " + \
                             str(SHSGoodHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(SHSGoodHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * SHSGoodHostTimeThreshold)) 
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " +  RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
		logger.debug("query is " + query)
                timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
			logger.debug("numberOfRestarts= " + str(numberOfRestarts))
                if numberOfRestarts < SHSGoodHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating SHS2 move to new host")
            # Check last SHSBadHostTimeThreshold min of time series data 
            # to see number or restart attempts 
            # if more than SHSBadHostRestartThreshold move the SHS.    
            else:
                logger.debug("Current SHS2 host health is BAD checking last " + \
                             str(SHSBadHostTimeThreshold) + \
                             " minutes of data to check if number of restart attempts are " + \
                             str(SHSBadHostRestartThreshold) + " or more")
                
                from_time = datetime.datetime.fromtimestamp(time.time() - (60 * SHSBadHostTimeThreshold))
                to_time = datetime.datetime.fromtimestamp(time.time())
                query = "SELECT sum(integral(unexpected_exits_rate)) WHERE roleType = " +  RoleType + " AND clusterDisplayName = '" + clusterDisplayName + "'"
                timeSeriesResult = api.query_timeseries(query, from_time, to_time)
                numberOfRestarts = 0.0
                for timeSeries in timeSeriesResult[0].timeSeries:
                    for point in timeSeries.data:
                        numberOfRestarts = point.value / 60
                if numberOfRestarts < SHSBadHostRestartThreshold:
                    SleepFlag = True
                    logger.debug("Current value under threshold skipping this iteration: " + str(numberOfRestarts))
                else:
                    logger.info("Current value above threshold initiating SHS2 move to new host")
                    
            # One of the above conditions is met and we need to move the SHS
            if not SleepFlag:
                break  
            
            # Sleep for monitoringTimeThreshold minute's and then again start monitoring 
            time.sleep(60 * monitoringTimeThreshold) 
                 
        except ApiException as err:
            logger.error("Error while monitoring retrying iteration after " + str(monitoringTimeThreshold) +  " min error: " + str(err))
            time.sleep(60 * monitoringTimeThreshold)

def moveSHS():
    logger.info("Starting SHS2 move to new host ")
    logger.info("Move reason " + str(currentSHSHostHealth \
                and "Good SHS2 Host health but reached threshold unexpected exits" \
                or "Bad SHS2 Host health and reached threshold unexpected exits"))
       
    moveReason = str(currentSHSHostHealth \
                and "Spark2 History server host is in Good health but unexpected exits reached threshold" \
                or "Spark2 History server host is in Bad health and unexpected exits reached threshold")  
    
    # Get latest state for cluster, spark service and SHS role    
    refreshGlobals()

    # Create a list of available hosts for new SHS
    logger.debug("Finding new host to move SHS2 to")
    newHosts = [SHSPrimaryHost] + SHSFailoverHosts
    SHSCurrentHost = api.get_host(currentSHS.hostRef.hostId).hostname.encode('utf-8')
    newHosts.remove(SHSCurrentHost)
    logger.debug("Possible hosts list: " + str(newHosts))
    
    # Check health for new hosts to find available hosts
    availableNewHosts = [] 
    for hostName in api.get_all_hosts():
        if hostName.hostname in newHosts:
            host = api.get_host(hostName.hostId)
            if host.healthSummary == "GOOD":
                availableNewHosts += [host]
    
    newSHSHost = None
    if availableNewHosts != []:
        logger.debug("Hosts found: " + str([host.hostname for host in availableNewHosts]))
        newSHSHost = availableNewHosts[0]
    
    # Send moving SHS email
    SendMail(Mode='Move', moveReason=moveReason, currentHost=SHSCurrentHost, newHost=(newSHSHost.hostname if newSHSHost != None else 'N/A'))
    
    if newSHSHost == None:
        logger.critical("No available host to relocate SHS2")
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="No available host to relocate SHS2" , failureErrorMessage="None")
        return 
    
    # Issue Stop command on current SHS role and wait for command to finish
    try:
        logger.info("Stopping current SHS2 role")
        stopCommand = sparkService.stop_roles(currentSHS.name)
        stopCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to stop current SHS2 role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to stop current SHS2 role" , failureErrorMessage=str(err))
        return 
    
    # Delete currentSHS role 
    try: 
        logger.info("Deleting current SHS2 role")
        sparkService.delete_role(currentSHS.name)
    except ApiException as err:
        logger.critical("Failed to delete current SHS2 role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to delete current SHS2 role" , failureErrorMessage=str(err))
        return
    
    # Create new SHS role on first host from available host list 
    newSHS = None
    try:
        if newSHSHost != None:
            logger.info("Moving SHS2 to new host: " + newSHSHost.hostname)
            newSHS = sparkService.create_role( sparkService.name + '-' + RoleType + '-' + str(int(time.time())), 
                                              RoleType, 
                                              newSHSHost.hostId)       
    except ApiException as err:
        logger.critical("Failed to create new SHS2 role > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to create new SHS2 role " , failureErrorMessage=str(err))
        return
    
    # Deploy new client configuration on cluster
    try:
        logger.info("Deploying new client configuration on cluster")
        newSHS.update_config({'unexpected_exits_thresholds':'{"warning":"never","critical":"never"}'})
        configDeployCommand = cluster.deploy_client_config()
        configDeployCommand.wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to deploy new configuration on cluster > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to deploy new configuration on cluster" , failureErrorMessage=str(err))
        return
    
    # Restart the new SHS role
    try:
        logger.info("Starting new SHS2")
        restartCommand = sparkService.restart_roles(newSHS.name)
        restartCommand[0].wait(commandTimeOut)
    except ApiException as err:
        logger.critical("Failed to restart new Spark2 History Server > " + str(err))
        if sendFailureEmail:
            SendMail(Mode="Failure", failureReason="Failed to restart new Spark2 History Server" , failureErrorMessage=str(err))
        return
    
    logger.info("SHS2 Move to new server completed successfully")
    if sendSuccessEmail:
        SendMail(Mode="Success")
    
if __name__ == '__main__':
    # create a process ID file in /var/run/shs2ha
    pid = os.getpid()
    op = open("/var/run/shs2ha","w")
    op.write("%s" % pid)
    op.close()

    logger.info("Starting script")
    # Get Cloudera manager api handle
    api = ApiResource(clouderaManagerHost, clouderaManagerPort, clouderaManagerUserName, clouderaManagerPassword, clouderaManagerHTTPS, clouderaManagerApiVersion)
    logger.info("Got API instance > " + str(api))
    # Monitor current status of Spark History Server and move to new host if required
    while True:
        # Monitor SHS state based on provided rules 
        monitorSHS()
        # Move SHS to a new host 
        moveSHS()
    
    
###############################
#SPARK2_YARN_HISTORY_SERVER kill
###############################
#!/bin/bash
echo "Running test kill script"
echo "Script terminates Job history server process"
echo "Process will be killed $1 time in every $2 seconds"
echo "-------------------------------------------------------------------------------"
for counter in $(seq 1 $1)
do
        echo "Starting iteration $counter: $(date)"
        kill -9 $(ps -Af | grep "SPARK2_YARN_HISTORY_SERVER" | grep -v "grep" | awk '{print $2}')
        echo "Process killed"
        echo "-------------------------------------------------------------------------------"
        sleep $2
done

###############################
#SPARK_YARN_HISTORY_SERVER kill
###############################
#!/bin/bash
echo "Running test kill script"
echo "Script terminates Job history server process"
echo "Process will be killed $1 time in every $2 seconds"
echo "-------------------------------------------------------------------------------"
for counter in $(seq 1 $1)
do
        echo "Starting iteration $counter: $(date)"
        kill -9 $(ps -Af | grep "SPARK_YARN_HISTORY_SERVER" | grep -v "grep" | awk '{print $2}')
        echo "Process killed"
        echo "-------------------------------------------------------------------------------"
        sleep $2
done

###############################
#JobHistoryServer kill
###############################
#!/bin/bash
echo "Running test kill script"
echo "Script terminates Job history server process"
echo "Process will be killed $1 time in every $2 seconds"
echo "-------------------------------------------------------------------------------"
for counter in $(seq 1 $1)
do
        echo "Starting iteration $counter: $(date)"
        kill -9 $(ps -Af | grep "JobHistoryServer" | grep -v "grep" | awk '{print $2}')
        echo "Process killed"
        echo "-------------------------------------------------------------------------------"
        sleep $2
done


###############################
#logging.conf 
###############################
[loggers]
keys=root,SHSLogger

[handlers]
keys=FileHandler

[formatters]
keys=fileFormatter

[logger_root]
level=DEBUG
handlers=FileHandler

[logger_SHSLogger]
level=DEBUG
handlers=FileHandler
qualname=SHS_HA
propagate=0

[handler_FileHandler]
class=handlers.RotatingFileHandler
level=DEBUG
formatter=fileFormatter
args=('/var/log/CATE/shsha/shs_ha.log','a',100000000,2)

[formatter_fileFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
datefmt=%m/%d/%Y %I:%M:%S %p

###############################
#application.conf 
###############################

[clouderaManager]
# Host name and port number to connect to Cloudera Manager  
clouderaManagerHost = CM_Host.nam.nsroot.net 
clouderaManagerPort = 7183
clouderaManagerApiVersion = 15

# Connecting Protocol for Cloudera manager (http = False, https = True)
clouderaManagerHTTPS = True

# Cloudera Manager local username and Password 
clouderaManagerUserName = admin
# Cloudera Manager password mode (Command, Plain)
clouderaManagerPasswordMode = Command
clouderaManagerPassword = sh /opt/cgfiles/common/bin/shs_ha/password.sh

# Cluster name as displayed on CM web page
clusterDisplayName = c5taco

[alertingService]
# Enable sending alert email 
sendAlertEmail = True 
# Send email on move success 
sendSuccessEmail = True
# Send email on move failure 
sendFailureEmail = True
# Email "CM restart" icon file path
alertEmailIcon = /opt/cgfiles/common/bin/shs_ha/configrestart.png
# If SMTP server uses TLS or not
alertEmailUseTLS = False
# To and from address for alert email
alertEmailFromAddress = email@email.com
alertEmailToAddress = email@email.com
# Address and port number of SMTP server used 
SMTPServerAddress = localhost
SMTPServerPort = 25
# Use authentication (False in case of SMTP server on localhost)
SMTPServerAuthentication = False
# SMTP server password mode (Command, Plain)
SMTPServerPasswordMode = Command
# Command to run for getting SMTP Server password  
SMTPServerPassword = sh /root/ha_jhs/TestPassword.sh

[sparkHistoryServer]
# Current / Primary host for Spark History Server
SHSPrimaryHost = shs_primary_host.nam.nsroot.net
# List of host that can be used in case of primary host fails
SHSFailoverHosts = shs_failover_host1.nam.nsroot.net, shs_failover_host2.nam.nsroot.net

[SHSFailover]
# Time in minute to check number of restart attempts while current Spark History Server's host is in good health  
SHSGoodHostTimeThreshold = 5
# Time in minute to check number of restart attempts while current Spark History Server's host is in bad health
SHSBadHostTimeThreshold = 2
# Number of restart attempts while current Spark History Server's host is in good health
SHSGoodHostRestartThreshold = 4 
# Number of restart attempts while current Spark History Server's host is in bad health
SHSBadHostRestartThreshold = 2

[FailoverScript]
# Scripts monitoring interval 
monitoringTimeThreshold = 1
# Time to wait for any command issues on cluster by this script 
commandTimeOut = 120
# Check for maintenance mode or not
maintenanceModeCheck = False
# Logging configuration file 
loggingConfigFile = /opt/cgfiles/common/bin/shs_ha/logging.conf
# Logger to use from this config file 
loggerName = SHSLogger


###############################
#cleanup.sh
# 클러스터 초기화 
###############################
#!/bin/bash

echo "Delete all cloudera related files on host: `hostname -f`"
echo "Will only delete files on local host runn this on all members to delete remote files"
echo "Make sure the cluster is stopped first"

read -r -p "Are you sure? [Y/n] " response
response=${response,,} # to lower

if [[ $response =~ ^(yes|y| ) ]]; then

    service cloudera-scm-agent stop
    service cloudera-scm-server stop
    service mysql stop

    # kill all cloudera proceses:
    kill -9 $(pgrep -f cmf)

    yum -y remove MySQL-client MySQL-server MySQL-shared-compat mysql-connector-java
    yum -y remove cloudera-manager-agent cloudera-manager-server cloudera-manager-daemons enterprise-debuginfo
    yum -y remove BD_cloudera_cdh_parcel*
    yum -y remove jdk
    yum -y remove BD_bigdata_repo

    #rm -Rf /opt/Cloudera
    rm -Rf /opt/cloudera/parcels
    rm -Rf /nn/* /data/[1-9]/*
    rm -Rf /var/lib/cloudera-scm-agent /var/lib/cloudera-scm-server
    rm -Rf /var/lib/mysql
    rm -Rf /var/lib/hue /var/lib/flume-ng /var/lib/hadoop-hdfs /var/lib/hadoop-yarn /var/lib/impala /var/lib/oozie /var/lib/zookeeper /var/lib/sqoop2
    rm -Rf /var/lib/cloudera*

fi  
