#################################################################################
	 2대의 클러스터의 환경설정을 비교해서 클러스터의 문제점 파악하는법
	 api_dump.py를 이용해서 클러스터의 모든 config 파일을 백업받는다.
	 api_dump.py는 cm_api를 이용하기 때문에 cm_api 패키지가 필요하다.
	 tar xvfz cm_api-15.0.0.tar.gz -C /tmp/
	 api_dump.py를 아래경로 밑에 카피해놓고 실행해야 한다.
	 /tmp/cm_api-12.0.0/src/
	 python api-dump.py -s <CM URL> -n <cluster_name> -p <패스워드> -o /tmp/dump
	 dump 받은 클러스터의 환경설정을 아래 cluster-compare.py를 이용해서 비교한다.
	 디렉토리를 지정하면 지정된 디렉토리의 모든 환경설정파일을 비교한다.
	 python cluster-compare.py /tmp/dump /tmp/api_dump
#################################################################################

#################################################################################
# api_dump.py 
python api-dump.py -s <CM URL> -n <cluster_name> -p <패스워드> -o /tmp/dump
#################################################################################

#!/usr/bin/env python
#coding:utf8
#cm_api.py 

import re, math, pprint, json, sys, argparse, os, ConfigParser, shutil,time
from cm_api.api_client import ApiResource
from subprocess import call

t = time.localtime()
timestamp = time.strftime('%b-%d-%Y_%H:%M', t)
debug = False
def buildParser(inputArgs):
    parser = argparse.ArgumentParser(
        description='Cloudera Manager Configuration APIs')

    parser.add_argument('-s', '--src', dest='cmHost', help='Source CM hostname')
    parser.add_argument('-P', '--port', dest='cmPort', default=7183, type=int, help='CM Port')
    parser.add_argument('-n', '--name', dest='clusterName',default='xxxxx', help='Source Cluster Name')
    parser.add_argument('-u', '--user', dest='user', default='admin', help='CM User')
    parser.add_argument('-p', '--pass', dest='password', help='CM Password')
    parser.add_argument('-v', dest='verbose', action='store_true', default=False,
                        help='Enable verbose logging')
    parser.add_argument('-o', '--output-directory', dest='outputDirname',
                        help='Dump the configuration to the specified local directory')
    return parser.parse_args(inputArgs)

def pickCluster(cList):
    # Print all cluster names and allow user to choose
    for i in xrange(len(cList)):
        print str(i) + " : " + cList[i].name + " / " + cList[i].fullVersion
    while True:
        try:
            cNum = int(raw_input("Pick the cluster number from above: "))
        except ValueError:
            print "Please provide a valid number from above"
            continue
        if cNum not in range(len(cList)):
            print "Please provide a valid number from above"
            continue
        else:
            if debug:
                print "Chosen cluster: " + cList[cNum].name
                print "Cluster version: " + cList[cNum].version
            return cList[cNum]

# Get all services for cluster
def getServices(cluster):
    """
    Gets an array of the configured services
    This assumes only 1 type of service per cluster.
    :param cluster
    :return: array of service datatypes
    Datastructure: service.name / .type
    If multiple services exists, add logic to copy a particular services to particular destination service
    """
    services = []
    for s in cluster.get_all_services():
        if debug:
            print s
        services.append(s)
    return services

def dumpConfig(cluster, dname):
    """
    :param cList: cluster list
    :param dname: output dirname
    This will dump the clusters configuration to a file per service.
    This is mainly used as a backup for cluster configuration.
    """
    services = getServices(cluster)
    # Iterate over services and print to file
    for s in services:
        out_conf = ConfigParser.RawConfigParser()
        out_conf.add_section(s.type)
        sConf = s.get_config(view='full')[0]
        if debug:
            print s.name
            pprint.pprint(sConf)
        for name, config in sConf.items():
            out_conf.set( s.type, config.name, config.value.encode('ascii','ignore').decode('ascii') if config.value else config.default )
        fname = "%s/%s.conf" % (dname,s.type.lower())
        with open(fname, "w") as out_file:
            out_conf.write(out_file)

        for group in s.get_all_role_config_groups():
            if debug:
                print "roleConfigGroup: " + group.name
            out_conf = ConfigParser.RawConfigParser()
            out_conf.add_section( group.roleType )
            gConf = group.get_config(view='full')
            for name, config in gConf.items():
                out_conf.set( group.roleType, config.name, config.value if config.value else config.default )
            fname = "%s/%s-%s.conf" % (dname,s.type.lower(), group.roleType.lower())
            with open(fname, "w") as out_file:
                out_conf.write(out_file)


if __name__ == "__main__":
    args = buildParser(sys.argv[1:])

    if args.verbose:
        debug = True

    if (args.cmHost is None):
        print "Must provide source hostnames for CM"
        exit(1)

    if (args.outputDirname is None):
        print "Must provide output directory"
        exit(2)

    if os.path.isdir( args.outputDirname ):
        print "Output directory already exists with configurations, will taking a backup of the directory which can be used for rolling back a config"
    else:
       os.mkdir( args.outputDirname)

    print "HELLO"
    print "host " + args.cmHost
    print "port " + str(args.cmPort)
    print "user " + args.user
    print "password " + args.password
    print "cluster " + args.clusterName

    sapi = ApiResource(args.cmHost, args.cmPort, args.user, args.password,use_tls=1)

    if (args.clusterName is None):
        # Get all cluster names
        sClusters = []
        for c in sapi.get_all_clusters():
            print c
            sClusters.append(c)

        # Choose source and destination clusters
        print "Source cluster: " + args.cmHost
        sCluster = pickCluster(sClusters)
    else:
        sCluster = sapi.get_cluster(args.clusterName)

    sName = args.outputDirname
    shutil.move(sName, sName +timestamp)
    os.mkdir(sName)
    dumpConfig(sCluster, sName)
    if os.path.exists(sName +"_edits"):
        shutil.move(sName +"_edits", sName+ "_edits" +timestamp)
        os.mkdir(sName +"_edits")
    else:
        shutil.copytree(sName, sName + "_edits")
		
#################################################################################
# cluster-compare.py 
  python cluster-compare.py /tmp/dump /tmp/api_dump
  python cluster-compare.py <환경설정 디렉토리1> <환경설정 디렉토리2>
#################################################################################
#!/usr/bin/python
# -*- coding: UTF-8 -*-
# Cluster-Compare script, adapted from a standard script to iterate through files. Modified
# to use difflib HTMl and generate a report to compare cluster configurations


import filecmp
import os
import os.path
import sys
import difflib
import subprocess
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

def diff(basepath, path1, path2):
    # STEP 1: iterate on all path1 paths (directories and filenames) and compare with path2
    # - report if path2 does not exist
    # - when path is a file, report if path1 differ from path2
    common_paths = []
    FILE='ccreport.html'
    if os.path.isfile(FILE) and os.access(FILE, os.R_OK):
       os.remove('ccreport.html')

    for path in os.listdir(path1):
        abs1 = os.path.join(path1, path)
        abs2 = os.path.join(path2, path)
        relative = abs1[len(basepath)+1:]
        if not os.path.exists(abs2):
             print "Only in %s: %s" % (os.path.dirname(abs1), path)
             continue
        common_paths.append(relative)
        if os.path.isfile(abs1):
            r = filecmp.cmp(abs1, abs2)
            if not r:
                #print "Files %s and %s differ" % (abs1, abs2)
                read1=open(abs1, 'U').readlines()
                read2=open(abs2, 'U').readlines()
                with open ('ccreport.html' ,'a') as f:
                        diff=difflib.HtmlDiff(tabsize=0.5).make_file(read1,read2,abs1,abs2,context="true")
                        f.writelines(diff)
        elif os.path.isdir(abs1):
            paths = diff(basepath, abs1, abs2)
            common_paths.extend(paths)

    # STEP 2: the other way: iterate on all path2 paths and
    # - report if path in path2 does not exist in path1
    for path in os.listdir(path2):
        abs1 = os.path.join(path1, path)
        abs2 = os.path.join(path2, path)
        relative = abs2[len(basepath)+1:]
        if not os.path.exists(abs1):
            print "Only in %s: %s" % (os.path.dirname(abs2), path)

    return common_paths
def email_send ():
        FILE='ccreport.html'
        email="jk83860@imcap.ap.ssmb.com"
        me="jk83860@imcap.ap.ssmb.com"
        msg = MIMEMultipart()
        msg['Subject'] = 'Cluster Comparison Report'
        msg['From']=me
        msg['To'] = email
        msg.preamble = 'Please find attached HTML report for Cluster comparison'
        f=file(FILE)
        attachment = MIMEText(f.read(),_subtype='html')
        attachment.add_header('Content-Disposition','attachment',filename="ccreport.html",FILE=FILE)
        msg.attach(attachment)
        s = smtplib.SMTP('localhost')
        s.sendmail(me,email, msg.as_string())
        s.quit()
if __name__ == "__main__":
    print len(sys.argv)
    if len(sys.argv) != 3:
        print "Usage: cluster-compare.py {path1} {path2} {email}"
        sys.exit()

    paths = sys.argv[1:]
    for path in paths:
        if not os.path.exists(path):
            print "%s does not exist!" % path
            sys.exit()
    basepath = os.path.commonprefix(paths).rstrip('/')

    diff(basepath, *paths)
    email_send()
	
	
#################################################################################
load-config.py 사용방법
update configuration

load-config.py –s <cmhost> -P <port> -u <username> -p <password> --original-directory <path-to-original-dir> 
--edits-directory <path-to-edits-dir> --filename <file> 
load-config.py –s <서버명> –P 7183 –u admin –p admin --original-directory config --edits-directory config_edits –f hdfs


명령어 수행후 configuration을 변경하겠냐는 질문이 나오면 y를 입력해 준다.

Do you want to commit these changes y/n? n
Please make the changes again and run the update again

Answering “y” will commit the change and deploy the client configuration along with restarting the service:

Do you want to commit these changes y/n? y
Deploying client configuration.....
Deployed client config
Restarting serivce...
Service successfully restarted



#################################################################################
#!/usr/bin/env python
# encoding: utf-8

import re, math, pprint, json, sys, argparse, os, ConfigParser, shutil
import difflib
from cm_api.api_client import ApiResource
from subprocess import call

# Assumes there is 1 type of service per cluster, i.e. 1 hive service on each cluster.
# Assumes there is at most 1 role group per role type in all services
CMD_TIMEOUT=180
debug = False
def buildParser(inputArgs):
    parser = argparse.ArgumentParser(
        description='Cloudera Manager Configuration APIs')
    parser.add_argument('-s', '--src', dest='cmHost', help='Source CM hostname')
    parser.add_argument('-P', '--port', dest='cmPort', default=7180, type=int, help='CM Port')
    parser.add_argument('-n', '--name', dest='clusterName', help='Source Cluster Name')
    parser.add_argument('-u', '--user', dest='user', default='admin', help='CM User')
    parser.add_argument('-p', '--pass', dest='password', default='admin', help='CM Password')
    parser.add_argument('-v', dest='verbose', action='store_true', default=False,
                        help='Enable verbose logging')
    parser.add_argument('--original-directory', dest='original_path',
                        help='Path to original output-directory')
    parser.add_argument('--edits-directory', dest='edits_path', help='Path to edits-directory')
    parser.add_argument('--filename', dest='filename', help='Filename to diff and update')
    return parser.parse_args(inputArgs)

def pickCluster(cList):
    # Print all cluster names and allow user to choose
    for i in xrange(len(cList)):
        print str(i) + " : " + cList[i].name + " / " + cList[i].fullVersion
    while True:
        try:
            cNum = int(raw_input("Pick the cluster number from above: "))
        except ValueError:
            print "Please provide a valid number from above"
            continue
        if cNum not in range(len(cList)):
            print "Please provide a valid number from above"
            continue
        else:
            if debug:
                print "Chosen cluster: " + cList[cNum].name
                print "Cluster version: " + cList[cNum].version
            return cList[cNum]

# Get all services for cluster
def getServices(cluster):
    """
    Gets an array of the configured services
    This assumes only 1 type of service per cluster.
    :param cluster
    :return: array of service datatypes
    Datastructure: service.name / .type
    If multiple services exists, add logic to copy a particular services to particular destination service
    """
    services = []
    for s in cluster.get_all_services():
        if debug:
            print s
        services.append(s)
    return services

def getIemToUpdate(cluster, service_role):
    dash_idx = service_role.find("-")
    if dash_idx == -1: # purely a service
        update_item = ("service", service_role)
    else: # service-role combo
        update_item = ("role", service_role[:dash_idx], service_role[dash_idx+1:])

    (service,role) = (None, None)
    for s in cluster.get_all_services():
        if s.type.lower() == update_item[1]:
            service = s
            break

    if update_item[0] == "service":
        return (service,role)

    for group in service.get_all_role_config_groups():
        if group.roleType.lower() == update_item[2]:
            return (service, group)

    raise Exception("Unable to find service/role in cluster")

def diff_load(cluster, service_role, original, edits):
    (service, role) = getIemToUpdate(cluster, service_role)

    edits_conf = ConfigParser.RawConfigParser()
    edits_conf.read(edits + "/" + service_role + ".conf")
    edits_dict = dict(edits_conf.items(edits_conf.sections()[0]))

    orig_conf = ConfigParser.RawConfigParser()
    orig_conf.read(original + "/" + service_role + ".conf")
    orig_dict = dict( orig_conf.items(orig_conf.sections()[0]) )

    print "The following changes have been made\n"
    diff_keys = []
    for k in set(orig_dict.keys())| set(edits_dict.keys()):
        if orig_dict[k] != edits_dict[k]:
            print "Key: %s, Original: [%s], Edit: [%s]" % (k, orig_dict[k], edits_dict[k])
            diff_keys.append(k)

    change=raw_input("Do you want to commit these changes y/n? ")
    if change != "y":
        print "Please make the changes again and run the update again"
        return

    update_item = role if role else service
    for k in diff_keys:
        v = edits_dict[k]
        if v not in "None":
            update_item.update_config({k: v})

    print "Deploying client configuration....."
    cmd = cluster.deploy_client_config()
    if not cmd.wait(CMD_TIMEOUT).success:
        print "Failed to deploy client configs"
    else:
        print "Deployed client config"
        print "Restarting serivce..."
        cmd = service.restart()
        if not cmd.wait(CMD_TIMEOUT).success:
            print "Failed to start service"
        else:
            print "Service successfully restarted"

if __name__ == "__main__":
    args = buildParser(sys.argv[1:])
    if args.verbose:
        debug = True

    if (args.cmHost is None):
        print "Must provide source hostnames for CM"
        exit(1)

    if args.original_path is None or args.edits_path is None:
        print "Must provide both original and edits directories"
        exit(2)

    if not os.path.isdir( args.original_path ) or not os.path.isdir( args.edits_path ):
        print "Both directories must exist"
        exit(3)

    sapi = ApiResource(args.cmHost, args.cmPort, args.user, args.password,use_tls=0)

    if (args.clusterName is None):
        # Get all cluster names
        sClusters = []
        for c in sapi.get_all_clusters():
            sClusters.append(c)

        # Choose source and destination clusters
        print "Source cluster: " + args.cmHost
        sCluster = pickCluster(sClusters)
    else:
        sCluster = sapi.get_cluster(args.cmHost)

    sName = args.filename
    diff_load(sCluster, sName, args.original_path, args.edits_path)
###################################################################################################
CLOUDERA REST API의 사용
<cluster name>은 대소문자를 구분한다.
패스워드 정보가 암호화 되어 나오지 않기때문에 패스워드 확인목적으로 사용하기 편리하다.
###################################################################################################
클러스터 정보 가져오기
===============================
curl -u admin:Cmpassdev1 -k https://<CM_SERVER_HOSTNAME>:7183/api/v9/clusters
{
  "items" : [ {
    "name" : "KRBDDEV",
    "displayName" : "KRBDDEV",
    "version" : "CDH5",
    "fullVersion" : "5.9.1",
    "maintenanceMode" : false,
    "maintenanceOwners" : [ ]
  } ]
  
클러스터 서비스 정보 가져오기  
================================
curl -u admin:Cmpassdev1 -k https://<CM_SERVER_HOSTNAME>:7183/api/v9/clusters/KRBDDEV/services

{
  "items" : [ {
    "name" : "zookeeper",
    "type" : "ZOOKEEPER",
    "clusterRef" : {
      "clusterName" : "KRBDDEV"
    },
    "serviceUrl" : "https://<CM_SERVER_HOSTNAME>:7183/cmf/serviceRedirect/zookeeper",
    "serviceState" : "STARTED",
    "healthSummary" : "GOOD",
    "healthChecks" : [ {
      "name" : "ZOOKEEPER_CANARY_HEALTH",
      "summary" : "GOOD"
    }, {
      "name" : "ZOOKEEPER_SERVERS_HEALTHY",
      "summary" : "GOOD"
    } ],
    "configStalenessStatus" : "FRESH",
    "clientConfigStalenessStatus" : "FRESH",
    "maintenanceMode" : false,
    "maintenanceOwners" : [ ],
    "displayName" : "zookeeper"
  }, {
    "name" : "spark2_on_yarn",
    "type" : "SPARK2_ON_YARN",
    "clusterRef" : {
      "clusterName" : "KRBDDEV"
    },
    "serviceUrl" : "https://<CM_SERVER_HOSTNAME>:7183/cmf/serviceRedirect/spark2_on_yarn",
    "serviceState" : "STARTED",
    "healthSummary" : "GOOD",
    "healthChecks" : [ ],
    "configStalenessStatus" : "FRESH",
    "clientConfigStalenessStatus" : "FRESH",
    "maintenanceMode" : false,
    "maintenanceOwners" : [ ],
    "displayName" : "Spark 2"
  } ]
}
클러스터 환경설정 가져오기  
================================
curl -u admin:Cmpassdev1 -k https://<CM_SERVER_HOSTNAME>:7183/api/v9/clusters/KRBDDEV/services/hdfs/config


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@ pyspark 정리
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
spark 실행하려면 cloudera-scm으로 로긴후 hive 티켓을 받은후 spark로 실행해야함.

##############################################################
# pyspark job 제출 예제
##############################################################
spark-submit --master yarn \
             --deploy-mode client \
			 --executor-memory 1g \
			 --name wordcount \
			 --conf "spark.app.id=wordcount" \
			 --wordcount.py /tmp/nba.txt 2  

##############################################################
# wordCounts
##############################################################		
lines=sc.textFile("/tmp/nba.txt")
lines.take(3)
#아래처럼 한줄이 리스트 원소 한개
[u'The first query shows that Curry makes, on average, more three-pointers than anyone in the history of the NBA. ', 
 u'The second query shows that Joe Hassett had the best three-point shooting season in 1981, ', 
 u'as compared to the rest of the league, in the history of the NBA. Curry doesnven rank in the top 10. (He barely misses it, coming in at 12th.) ']

words_list=lines.flatMap(lambda x: x.split(" "))
words_list.take(10)
#단어가 리스트 원소 한개
[u'The', u'first', u'query', u'shows', u'that', 
 u'Curry', u'makes,', u'on', u'average,', u'more']

one_list=words_list.map(lambda x : (x, 1))
one_list.collect()
#딕셔너리 형식으로 단어에 숫자 1을 붙임
[(u'The', 1), (u'first', 1), (u'query', 1), (u'shows', 1), (u'that', 1), 
 (u'Curry', 1), (u'makes,', 1), (u'on', 1), (u'average,', 1), (u'more', 1)]

word_count=one_list.reduceByKey(lambda x, y: (x +y))
word_count.take(5)
#단어별로 횟수를 합함
[(u'', 6), (u'1981,', 2), (u'shot', 1), (u'is', 2), (u'rest', 1)]

#키값으로 소트 asc
sorted = word_count.sortByKey()

#키값으로 소트 desc
sorted = word_count.sortByKey(false)			 
			 
##############################################################
# 다양한 자료 타입에 대한 aggregate
##############################################################	
people = []
people.append({'name':'Bob', 'age':45,'gender':'M'})
people.append({'name':'Gloria', 'age':43,'gender':'F'})
people.append({'name':'Albert', 'age':28,'gender':'M'})
people.append({'name':'Laura', 'age':33,'gender':'F'})
people.append({'name':'Simone', 'age':18,'gender':'T'})
peopleRdd=sc.parallelize(people)
len(peopleRdd.collect())
seqOp = (lambda x,y: (x[0] + y['age'],x[1] + 1))
combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))
peopleRdd.aggregate((0,0), seqOp, combOp)
seqOp = (lambda x,y: (x[0] + y[2],x[1] + 1))
userRdd.aggregate((0,0), seqOp, combOp)


csv 데이터에 movie user data에 적용
user_lines = sc.textFile("/tmp/users.dat")
user_data=user_lines.map(lambda x: x.split("|"))  
#아래에서 int(y[2])를 쓴이유는 age 가 3번째 인덱스고 문자열 타입이라 형변환이 필요한다.
seqOp = (lambda x,y: (x[0] + int(y[2]),x[1] + 1))
combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))
user_data.aggregate((0,0), seqOp, combOp)


# aggregate로 평균구하기
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount=rdd.aggregate((0,0),(lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2:(acc1[0] + acc2[0], acc1[1] + acc2[1])))
>>> print sumCount
(55, 10)			 
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# fold를 이용한 평균구하기
##############################################################	
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount = rdd.map(lambda x: (x, 1)).fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# reduce를 이용한 평균구하기
##############################################################
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount=rdd.map(lambda x: (x, 1)).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# pair RDD 평균구하기
##############################################################

rdd=sc.parallelize([('panda', 0),('pink', 3),('pirate', 3),('panda', 1),('pink', 4)])
>>> avg_rdd=rdd.mapValues(lambda x: (x, 1))
>>> avg_rdd.take(10)
[('panda', (0, 1)), ('pink', (3, 1)), ('pirate', (3, 1)), ('panda', (1, 1)), ('pink', (4, 1))]
>>> reduce_rdd=avg_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
>>> reduce_rdd.take(10)
[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]

##############################################################
# pair RDD 평균구하기 함수
##############################################################
import sys
from pyspark import SparkContext

def basicAvg(nums):
    """Compute the avg"""
    sumCount = nums.map(lambda x: (x, 1)).fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
    return sumCount[0] / float(sumCount[1])

if __name__ == "__main__":
    master = "local"
    if len(sys.argv) == 2:
        master = sys.argv[1]
    sc = SparkContext(master, "Sum")
    nums = sc.parallelize([1, 2, 3, 4])
    avg = basicAvg(nums)
    print avg



##############################################################
# 집합함수 distinct union intersection subtract
##############################################################
distinct()         : 유일한 값만 추출한다.
>>>rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd.distinct().collect()
[2, 4, 1, 3]  

union              : 데이터셋을 합한다.
>>> rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd2 = sc.parallelize([8,9,0])
>>> rdd.union(rdd2).collect()
[1, 2, 3, 4, 4, 1, 2, 8, 9, 0] 

intersection       : 데이터셋의 교집합을 구한다.
>>> rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd2 = sc.parallelize([8,9,0])
>>> rdd2 = sc.parallelize([8,9,0,1,4])
>>> rdd2.intersection(rdd).collect()
[4, 1] 

subtract()         : 한 rdd에서 다른 rdd가 포함하는 원소를 삭제한다.
>>> rdd.subtract(rdd2).collect()
[2, 2, 3] 

##############################################################
# pair RDD
# CSV 파일에서 키값과 나머지 컬럼을 분리 하는 방법
##############################################################			 
user_data=user_lines.map(lambda x: (x.split("|")[0],x.split("|")))			 
			 
####################################################################################
# RDD 함수의 종류
####################################################################################
collect()     : RDD를 적용시킨 리스트를 반환한다.
ex)
lines=sc.textFile("/tmp/nba.txt")
lines.collect()

count()       : RDD 요소의 갯수를 반환한다.
ex)
words_lists=lines.flatMap(lambda x: x.split(" "))
words_lists.collect()

filter()      : RDD에서 참인 경우만 요소를 통과시켜 새로운 RDD 반환한다.
ex)
lines = sc.textFile("/tmp/users.dat")
csv_data=lines.map(lambda x: x.split("|"))  
csv_data.count()
male_list=csv_data.filter(lambda x: x[1]=="M")

first()       : RDD에서 첫번째 리스트의 값을 반환한다.
                리스트에 들어있는 요소에 따라 type이 다른다.
ex)				
lines = sc.textFile("/tmp/users.dat")
lines.first()				

take()        : RDD에서 입력 파라메터 크기 만큼 리스트를 반환한다.
ex)				
lines = sc.textFile("/tmp/users.dat")
lines.take(10)	

reduce()      : RDD에서 결과값을 하나로 합쳐주는 역할을 한다.
ex)
male_list=csv_data.filter(lambda x: x[1]=="M")
male_list.count()
male_age_list=male_list.map(lambda x: int(x[2]))
male_total=male_age_list.reduce(lambda x,y: x + y)

sortByKey()   : key:value 구조에서 키값 기준으로 데이터를 소트한다.
                key:value 구조가 아니면 사용할 수 없다.

top()         : 원하는 갯수만큼 sort한 결과 값을 져온다.
                -가  오름차순이다.
ex)
male_age_min =male_age_list.top(10,lambda s: +s)

takeOrdered() : 원하는 갯수만큼 sort한 결과 값을 져온다.
			    +가  오름차순이다.
ex)
male_age_min =male_age_list.takeOrdered(1,lambda s: +s)		
#multifield orderby
movie_count.takeOrdered(10, key=lambda x: (-1 * x[0],x[1]))		
				
takeSample()  :  random한 요소를 반환한다.
ex)
male_age_sample =male_age_list.takeSample(withReplacement=True, num=6, seed=1000)
male_age_sample =male_age_list.takeSample(True, 6, 1000)

countByValue():  값별로 갯수를 반환한다.
flatMap()     : map 함수가 수행될때 때때로 list나 튜플 형태로 반환될 때가 있다.
                list안의 값을 모두 나열하여 새로운 RDD를 구성하기를 원할때 사용한다.
reduceByKey() : pair RDD를 이루고 있는 경우 적용이 가능하다
                key값을 기준으로 셔플이 하고 이를 합산을 해준다.
groupByKey()      :  pair RDD를 이루고 있는 경우 적용이 가능하다
                     key, value값을 기준으로 셔플이 하고 합산을 해준다.
				     key가 같은데 value가 다르면 불필요한 셔플이 일어나
				     네트웍, 메모리에 부하를 준다.
				     같은 키값이 많은 경우 oom이 발생할 수 있다.
cache() 함수      : RDD를 메모리에 상주시키는 역할을 한다.

persist()         : RDD를 메모리에 상주시키는 역할을 한다.

unpersist()       : 메모리에서 해당 데이터를 제거

toDebugString()   : 현재 생성된 RDD의 정보를 보여준다.
                  (2) male_age_listRDD PythonRDD[12] at RDD at PythonRDD.scala:43 [Memory Serialized 1x Replicated]
                  |  /tmp/users.dat MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]
                  |  /tmp/users.dat HadoopRDD[7] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]
getStorageLevel() : RDD가 현재 어느위치에 저장되어 있는지(메모리 or 디스크)를 보여준다.
                    Memory Serialized 1x Replicated 
					
union()           : 두개의 RDD를 합할대 사용한다.

getNumPartitions(): RDD의 현재 파티션의 수를 알아볼때 사용

repartition()     : 대형 파티션에서 데이터 필터링 후 불필요한 셔플을 방지하기 위해
                    파티션의 수를 줄이는데 사용합니다.
					
coalesce()        : 대형 파티션에서 데이터 필터링 후 불필요한 셔플을 방지하기 위해
                    파티션의 수를 줄이는데 사용합니다.
					repartition의 최적화된 버전입니다.

					




####################################################################################
# pyspark 특정 컬럼의 합계 count min max
####################################################################################
lines = sc.textFile("/tmp/users.dat")
csv_data=lines.map(lambda x: x.split("|"))  
csv_data.count()
#성별 남자의 나이의 합계
male_list=csv_data.filter(lambda x: x[1]=="M")
male_list.count()
male_age_list=male_list.map(lambda x: int(x[2]))
male_total=male_age_list.reduce(lambda x,y: x + y)

#성별 여자의 나이의 합계
female_list=csv_data.filter(lambda x: x[1]=="F")
female_list.count()
female_age_list=female_list.map(lambda x: int(x[2]))
# 아래와 같이 reduce 또는 fold를 사용할 수 있다.
female_total=female_age_list.reduce(lambda x,y: x + y)
female_fold_total = female_age_list.fold(0, (lambda x, y: x + y))

female_fold_total = female_age_list.fold(0,add)

>>> rdd=sc.parallelize([("a",1),("b",2),("a",2)])
>>> from operator import add
>>> sorted(rdd.foldByKey(0,add).collect())
[('a', 3), ('b', 2)]

#성별 남자의 나이의 평균
#합산한 후 모수를 나눠주기위해 갯수도 같이 구한다.
male_age_sum = male_age_list.map(lambda x: (x, 1))
male_age_sum_cnt=male_age_sum.fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
male_age_sum_cnt[0] / float(male_age_sum_cnt[1])

#first와 take
#first와 take는 RDD가 얼마나 파티션닝되었냐에 따라 결과 값이 달라진다.
male_age_list


#sort : takeOrdered, top ,sortByKey
#sortByKey()는 key:value 구조가 아니면 사용할 수 없다.
#top은 -가 오름차순이다.
#takeOrdered +가 오름차순이다.
# 아래는 male_age_list에서 1000개를 추출하여 내림차순으로 정렬한다.
male_age_order =male_age_list.takeOrdered(1000,lambda s: -s)

#MAX
# 아래는 male_age_list에서 내림차순으로 정렬하므로 첫번째를 가져오면 max가 된다.
male_age_max =male_age_list.takeOrdered(1,lambda s: -s)

#MIN
# 아래는 male_age_list에서 오름차순으로 정렬하므로 첫번째를 가져오면 min 된다.
male_age_min =male_age_list.takeOrdered(1,lambda s: +s)

#TOP N
# 아래는 male_age_list에서 내림으로 정렬하므로 위서 부터 10개를 구하면 TOP 10이된다.
male_age_min =male_age_list.takeOrdered(10,lambda s: -s)

#takeSample
male_age_sample =male_age_list.takeSample(withReplacement=True, num=6, seed=1000)

#cache()
male_age_list.setName("male_age_listRDD")
male_age_list.cache()
print male_age_list.is_cached

#toDebugString() 
print male_age_list.toDebugString()

#getStorageLevel() 
print male_age_list.getStorageLevel()
4|Citi Revolving Cards|||072456447|0029064440500|4743604020611698|0029064440500|0|090828|2016-04-01|6000|6000|KRW||KOR|Korea, Republic of||N||N||2016040109082847436040206116980000000742950|鍮檣곕㏏?痢腎ㅺ뎄紳ㅻ|竊竊踰吏||||||100180||00074295|A|2016040109082847436040206116980000000742950||||||072456447||||||||||||||APR|2016-04-30|HCAS|||
####################################################################################
# pyspark SQLContext 사용하여 분석하기
####################################################################################

from pyspark.sql import SQLContext
from pyspark.sql import Row
from time import time 
sqlContext = SQLContext(sc)

lines= sc.textFile("/tmp/users.dat")

lines= sc.textFile("/tmp/AMLCARDS_HCAS_AUTHORIZATION_20160430_20170427.DAT")
lines= sc.textFile("file:///tmp/ttt.txt")
lines= sc.textFile("file:///tmp/ggg.txt")
csv_data = lines.map(lambda l: l.split("|"))
csv_data = lines.map(lambda l: l.split(","))
row_data = csv_data.map(lambda p: Row(id=int(p[0]),     
                                      gender=p[1],    
									  age=int(p[2]),    
									  zipcode=p[3]    
									  )
						)
row_data = csv_data.map(lambda p: Row(test=p[1]))	
auth_df=sqlContext.createDataFrame(row_data)				
user_df=sqlContext.createDataFrame(row_data)

#DF에 직접 SQL 날리기와 수행시간 계산
start=time()
user_df.select("id","gender").groupby("gender").count().show()
elapsed= time()-start
print "Query performed in %s seconds " % round(elapsed,3)

#groupby 절에 나오는 컬럼은 select 절에 있어야 함.
#user_df.select("age","gender").groupby("gender","age").count().show()
user_df

#아래처럼 테이블로 등록해서 사용할 수 도 있다.
user_df.registerTempTable("users")
auth_df.registerTempTable("auth")

#SQL의 수행결과는 RDD이다.
male_list=sqlContext.sql("""
select * from users where gender="M"
""")

user_list=sqlContext.sql("""
select * from users
""")

auth_list.show()

user_list.show()


male_list.show()

#데이터 프레임의 구조를 볼 수 있다.
user_df.printSchema()

####################################################################################
# pyspark
# spark sql join
####################################################################################
from pyspark import SparkContext, SparkConf, HiveContext

if __name__ == "__main__":

  # create Spark context with Spark configuration
  conf = (SparkConf().setMaster("local").setAppName("My app").set("spark.executor.memory", "1g"))
  sc = SparkContext(conf = conf)
  sqlContext = HiveContext(sc)
  df_07 = sqlContext.sql("SELECT * from sample_07")
  df_07.filter(df_07.salary > 150000).show()
  df_08 = sqlContext.sql("SELECT * from sample_08")
  tbls = sqlContext.sql("show tables")
  tbls.show()
  df_09 = df_07.join(df_08, df_07.code == df_08.code).select(df_07.code,df_07.description)
  df_09.show()
  df_09.write.saveAsTable("sample_09")
  tbls = sqlContext.sql("show tables")
  tbls.show()
  
####################################################################################
# outliers 설정
# www.dezyre.com 보고 정리 할것
####################################################################################

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@spark scala 정리
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  
 spark 실행하려면 cloudera-scm으로 로긴후 hive 티켓을 받은후 spark로 실행해야함.

##############################################################
# pyspark job 제출 예제
##############################################################
spark-submit --master yarn \
             --deploy-mode client \
			 --executor-memory 1g \
			 --name wordcount \
			 --conf "spark.app.id=wordcount" \
			 --wordcount.py /tmp/nba.txt 2  

##############################################################
# wordCounts
##############################################################		
lines=sc.textFile("/tmp/nba.txt")
lines.take(3)
#아래처럼 한줄이 리스트 원소 한개
[u'The first query shows that Curry makes, on average, more three-pointers than anyone in the history of the NBA. ', 
 u'The second query shows that Joe Hassett had the best three-point shooting season in 1981, ', 
 u'as compared to the rest of the league, in the history of the NBA. Curry doesnven rank in the top 10. (He barely misses it, coming in at 12th.) ']

words_list=lines.flatMap(lambda x: x.split(" "))
words_list.take(10)
#단어가 리스트 원소 한개
[u'The', u'first', u'query', u'shows', u'that', 
 u'Curry', u'makes,', u'on', u'average,', u'more']

one_list=words_list.map(lambda x : (x, 1))
one_list.collect()
#딕셔너리 형식으로 단어에 숫자 1을 붙임
[(u'The', 1), (u'first', 1), (u'query', 1), (u'shows', 1), (u'that', 1), 
 (u'Curry', 1), (u'makes,', 1), (u'on', 1), (u'average,', 1), (u'more', 1)]

word_count=one_list.reduceByKey(lambda x, y: (x +y))
word_count.take(5)
#단어별로 횟수를 합함
[(u'', 6), (u'1981,', 2), (u'shot', 1), (u'is', 2), (u'rest', 1)]

#키값으로 소트 asc
sorted = word_count.sortByKey()

#키값으로 소트 desc
sorted = word_count.sortByKey(false)			 
			 
##############################################################
# 다양한 자료 타입에 대한 aggregate
##############################################################	
people = []
people.append({'name':'Bob', 'age':45,'gender':'M'})
people.append({'name':'Gloria', 'age':43,'gender':'F'})
people.append({'name':'Albert', 'age':28,'gender':'M'})
people.append({'name':'Laura', 'age':33,'gender':'F'})
people.append({'name':'Simone', 'age':18,'gender':'T'})
peopleRdd=sc.parallelize(people)
len(peopleRdd.collect())
seqOp = (lambda x,y: (x[0] + y['age'],x[1] + 1))
combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))
peopleRdd.aggregate((0,0), seqOp, combOp)
seqOp = (lambda x,y: (x[0] + y[2],x[1] + 1))
userRdd.aggregate((0,0), seqOp, combOp)


csv 데이터에 movie user data에 적용
user_lines = sc.textFile("/tmp/users.dat")
user_data=user_lines.map(lambda x: x.split("|"))  
#아래에서 int(y[2])를 쓴이유는 age 가 3번째 인덱스고 문자열 타입이라 형변환이 필요한다.
seqOp = (lambda x,y: (x[0] + int(y[2]),x[1] + 1))
combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))
user_data.aggregate((0,0), seqOp, combOp)


# aggregate로 평균구하기
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount=rdd.aggregate((0,0),(lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2:(acc1[0] + acc2[0], acc1[1] + acc2[1])))
>>> print sumCount
(55, 10)			 
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# fold를 이용한 평균구하기
##############################################################	
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount = rdd.map(lambda x: (x, 1)).fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# reduce를 이용한 평균구하기
##############################################################
>>> rdd=sc.parallelize([1,2,3,4,5,6,7,8,9,10])
>>> sumCount=rdd.map(lambda x: (x, 1)).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))
>>> print sumCount[0] / float(sumCount[1])
5.5

##############################################################
# pair RDD 평균구하기
##############################################################

rdd=sc.parallelize([('panda', 0),('pink', 3),('pirate', 3),('panda', 1),('pink', 4)])
>>> avg_rdd=rdd.mapValues(lambda x: (x, 1))
>>> avg_rdd.take(10)
[('panda', (0, 1)), ('pink', (3, 1)), ('pirate', (3, 1)), ('panda', (1, 1)), ('pink', (4, 1))]
>>> reduce_rdd=avg_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
>>> reduce_rdd.take(10)
[('pink', (7, 2)), ('panda', (1, 2)), ('pirate', (3, 1))]

##############################################################
# pair RDD 평균구하기 함수
##############################################################
import sys
from pyspark import SparkContext

def basicAvg(nums):
    """Compute the avg"""
    sumCount = nums.map(lambda x: (x, 1)).fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
    return sumCount[0] / float(sumCount[1])

if __name__ == "__main__":
    master = "local"
    if len(sys.argv) == 2:
        master = sys.argv[1]
    sc = SparkContext(master, "Sum")
    nums = sc.parallelize([1, 2, 3, 4])
    avg = basicAvg(nums)
    print avg



##############################################################
# 집합함수 distinct union intersection subtract
##############################################################
distinct()         : 유일한 값만 추출한다.
>>>rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd.distinct().collect()
[2, 4, 1, 3]  

union              : 데이터셋을 합한다.
>>> rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd2 = sc.parallelize([8,9,0])
>>> rdd.union(rdd2).collect()
[1, 2, 3, 4, 4, 1, 2, 8, 9, 0] 

intersection       : 데이터셋의 교집합을 구한다.
>>> rdd = sc.parallelize([1,2,3,4,4,1,2])
>>> rdd2 = sc.parallelize([8,9,0])
>>> rdd2 = sc.parallelize([8,9,0,1,4])
>>> rdd2.intersection(rdd).collect()
[4, 1] 

subtract()         : 한 rdd에서 다른 rdd가 포함하는 원소를 삭제한다.
>>> rdd.subtract(rdd2).collect()
[2, 2, 3] 

##############################################################
# pair RDD
# CSV 파일에서 키값과 나머지 컬럼을 분리 하는 방법
##############################################################			 
user_data=user_lines.map(lambda x: (x.split("|")[0],x.split("|")))			 
			 
####################################################################################
# RDD 함수의 종류
####################################################################################
collect()     : RDD를 적용시킨 리스트를 반환한다.
ex)
lines=sc.textFile("/tmp/nba.txt")
lines.collect()

count()       : RDD 요소의 갯수를 반환한다.
ex)
words_lists=lines.flatMap(lambda x: x.split(" "))
words_lists.collect()

filter()      : RDD에서 참인 경우만 요소를 통과시켜 새로운 RDD 반환한다.
ex)
lines = sc.textFile("/tmp/users.dat")
csv_data=lines.map(lambda x: x.split("|"))  
csv_data.count()
male_list=csv_data.filter(lambda x: x[1]=="M")

first()       : RDD에서 첫번째 리스트의 값을 반환한다.
                리스트에 들어있는 요소에 따라 type이 다른다.
ex)				
lines = sc.textFile("/tmp/users.dat")
lines.first()				

take()        : RDD에서 입력 파라메터 크기 만큼 리스트를 반환한다.
ex)				
lines = sc.textFile("/tmp/users.dat")
lines.take(10)	

reduce()      : RDD에서 결과값을 하나로 합쳐주는 역할을 한다.
ex)
male_list=csv_data.filter(lambda x: x[1]=="M")
male_list.count()
male_age_list=male_list.map(lambda x: int(x[2]))
male_total=male_age_list.reduce(lambda x,y: x + y)

sortByKey()   : key:value 구조에서 키값 기준으로 데이터를 소트한다.
                key:value 구조가 아니면 사용할 수 없다.

top()         : 원하는 갯수만큼 sort한 결과 값을 져온다.
                -가  오름차순이다.
ex)
male_age_min =male_age_list.top(10,lambda s: +s)

takeOrdered() : 원하는 갯수만큼 sort한 결과 값을 져온다.
			    +가  오름차순이다.
ex)
male_age_min =male_age_list.takeOrdered(1,lambda s: +s)		
#multifield orderby
movie_count.takeOrdered(10, key=lambda x: (-1 * x[0],x[1]))		
				
takeSample()  :  random한 요소를 반환한다.
ex)
male_age_sample =male_age_list.takeSample(withReplacement=True, num=6, seed=1000)
male_age_sample =male_age_list.takeSample(True, 6, 1000)

countByValue():  값별로 갯수를 반환한다.
flatMap()     : map 함수가 수행될때 때때로 list나 튜플 형태로 반환될 때가 있다.
                list안의 값을 모두 나열하여 새로운 RDD를 구성하기를 원할때 사용한다.
reduceByKey() : pair RDD를 이루고 있는 경우 적용이 가능하다
                key값을 기준으로 셔플이 하고 이를 합산을 해준다.
groupByKey()      :  pair RDD를 이루고 있는 경우 적용이 가능하다
                     key, value값을 기준으로 셔플이 하고 합산을 해준다.
				     key가 같은데 value가 다르면 불필요한 셔플이 일어나
				     네트웍, 메모리에 부하를 준다.
				     같은 키값이 많은 경우 oom이 발생할 수 있다.
cache() 함수      : RDD를 메모리에 상주시키는 역할을 한다.

persist()         : RDD를 메모리에 상주시키는 역할을 한다.

unpersist()       : 메모리에서 해당 데이터를 제거

toDebugString()   : 현재 생성된 RDD의 정보를 보여준다.
                  (2) male_age_listRDD PythonRDD[12] at RDD at PythonRDD.scala:43 [Memory Serialized 1x Replicated]
                  |  /tmp/users.dat MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]
                  |  /tmp/users.dat HadoopRDD[7] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]
getStorageLevel() : RDD가 현재 어느위치에 저장되어 있는지(메모리 or 디스크)를 보여준다.
                    Memory Serialized 1x Replicated 
					
union()           : 두개의 RDD를 합할대 사용한다.

getNumPartitions(): RDD의 현재 파티션의 수를 알아볼때 사용

repartition()     : 대형 파티션에서 데이터 필터링 후 불필요한 셔플을 방지하기 위해
                    파티션의 수를 줄이는데 사용합니다.
					
coalesce()        : 대형 파티션에서 데이터 필터링 후 불필요한 셔플을 방지하기 위해
                    파티션의 수를 줄이는데 사용합니다.
					repartition의 최적화된 버전입니다.

					




####################################################################################
# pyspark 특정 컬럼의 합계 count min max
####################################################################################
lines = sc.textFile("/tmp/users.dat")
csv_data=lines.map(lambda x: x.split("|"))  
csv_data.count()
#성별 남자의 나이의 합계
male_list=csv_data.filter(lambda x: x[1]=="M")
male_list.count()
male_age_list=male_list.map(lambda x: int(x[2]))
male_total=male_age_list.reduce(lambda x,y: x + y)

#성별 여자의 나이의 합계
female_list=csv_data.filter(lambda x: x[1]=="F")
female_list.count()
female_age_list=female_list.map(lambda x: int(x[2]))
# 아래와 같이 reduce 또는 fold를 사용할 수 있다.
female_total=female_age_list.reduce(lambda x,y: x + y)
female_fold_total = female_age_list.fold(0, (lambda x, y: x + y))

female_fold_total = female_age_list.fold(0,add)

>>> rdd=sc.parallelize([("a",1),("b",2),("a",2)])
>>> from operator import add
>>> sorted(rdd.foldByKey(0,add).collect())
[('a', 3), ('b', 2)]

#성별 남자의 나이의 평균
#합산한 후 모수를 나눠주기위해 갯수도 같이 구한다.
male_age_sum = male_age_list.map(lambda x: (x, 1))
male_age_sum_cnt=male_age_sum.fold((0, 0), (lambda x, y: (x[0] + y[0], x[1] + y[1])))
male_age_sum_cnt[0] / float(male_age_sum_cnt[1])

#first와 take
#first와 take는 RDD가 얼마나 파티션닝되었냐에 따라 결과 값이 달라진다.
male_age_list


#sort : takeOrdered, top ,sortByKey
#sortByKey()는 key:value 구조가 아니면 사용할 수 없다.
#top은 -가 오름차순이다.
#takeOrdered +가 오름차순이다.
# 아래는 male_age_list에서 1000개를 추출하여 내림차순으로 정렬한다.
male_age_order =male_age_list.takeOrdered(1000,lambda s: -s)

#MAX
# 아래는 male_age_list에서 내림차순으로 정렬하므로 첫번째를 가져오면 max가 된다.
male_age_max =male_age_list.takeOrdered(1,lambda s: -s)

#MIN
# 아래는 male_age_list에서 오름차순으로 정렬하므로 첫번째를 가져오면 min 된다.
male_age_min =male_age_list.takeOrdered(1,lambda s: +s)

#TOP N
# 아래는 male_age_list에서 내림으로 정렬하므로 위서 부터 10개를 구하면 TOP 10이된다.
male_age_min =male_age_list.takeOrdered(10,lambda s: -s)

#takeSample
male_age_sample =male_age_list.takeSample(withReplacement=True, num=6, seed=1000)

#cache()
male_age_list.setName("male_age_listRDD")
male_age_list.cache()
print male_age_list.is_cached

#toDebugString() 
print male_age_list.toDebugString()

#getStorageLevel() 
print male_age_list.getStorageLevel()
4|Citi Revolving Cards|||072456447|0029064440500|4743604020611698|0029064440500|0|090828|2016-04-01|6000|6000|KRW||KOR|Korea, Republic of||N||N||2016040109082847436040206116980000000742950|鍮檣곕㏏?痢腎ㅺ뎄紳ㅻ|竊竊踰吏||||||100180||00074295|A|2016040109082847436040206116980000000742950||||||072456447||||||||||||||APR|2016-04-30|HCAS|||
####################################################################################
# pyspark SQLContext 사용하여 분석하기
####################################################################################

from pyspark.sql import SQLContext
from pyspark.sql import Row
from time import time 
sqlContext = SQLContext(sc)

lines= sc.textFile("/tmp/users.dat")

lines= sc.textFile("/tmp/AMLCARDS_HCAS_AUTHORIZATION_20160430_20170427.DAT")
lines= sc.textFile("file:///tmp/ttt.txt")
lines= sc.textFile("file:///tmp/ggg.txt")
csv_data = lines.map(lambda l: l.split("|"))
csv_data = lines.map(lambda l: l.split(","))
row_data = csv_data.map(lambda p: Row(id=int(p[0]),     
                                      gender=p[1],    
									  age=int(p[2]),    
									  zipcode=p[3]    
									  )
						)
row_data = csv_data.map(lambda p: Row(test=p[1]))	
auth_df=sqlContext.createDataFrame(row_data)				
user_df=sqlContext.createDataFrame(row_data)

#DF에 직접 SQL 날리기와 수행시간 계산
start=time()
user_df.select("id","gender").groupby("gender").count().show()
elapsed= time()-start
print "Query performed in %s seconds " % round(elapsed,3)

#groupby 절에 나오는 컬럼은 select 절에 있어야 함.
#user_df.select("age","gender").groupby("gender","age").count().show()
user_df

#아래처럼 테이블로 등록해서 사용할 수 도 있다.
user_df.registerTempTable("users")
auth_df.registerTempTable("auth")

#SQL의 수행결과는 RDD이다.
male_list=sqlContext.sql("""
select * from users where gender="M"
""")

user_list=sqlContext.sql("""
select * from users
""")

auth_list.show()

user_list.show()


male_list.show()

#데이터 프레임의 구조를 볼 수 있다.
user_df.printSchema()

####################################################################################
# pyspark
# spark sql join
####################################################################################
from pyspark import SparkContext, SparkConf, HiveContext

if __name__ == "__main__":

  # create Spark context with Spark configuration
  conf = (SparkConf().setMaster("local").setAppName("My app").set("spark.executor.memory", "1g"))
  sc = SparkContext(conf = conf)
  sqlContext = HiveContext(sc)
  df_07 = sqlContext.sql("SELECT * from sample_07")
  df_07.filter(df_07.salary > 150000).show()
  df_08 = sqlContext.sql("SELECT * from sample_08")
  tbls = sqlContext.sql("show tables")
  tbls.show()
  df_09 = df_07.join(df_08, df_07.code == df_08.code).select(df_07.code,df_07.description)
  df_09.show()
  df_09.write.saveAsTable("sample_09")
  tbls = sqlContext.sql("show tables")
  tbls.show()
  
####################################################################################
# outliers 설정
# www.dezyre.com 보고 정리 할것
####################################################################################
  
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# sqoop run book
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#RunSqoopOrchestration.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

# TALEND_PROJ_DIR=/opt/cgfiles/common/SqoopFrameWork/SqoopFrameWork
#
# export TALEND_PROJ_DIR

# NOW=`date '+%Y%m%d_%H%M%S'`
# export NOW

# JAVA_HOME=/usr/java/jdk1.6.0_31
# export JAVA_HOME

# export PATH=$JAVA_HOME/bin:$PATH

# TALEND_LOG=/var/log/talend
# export TALEND_LOG

# export id_in_num=`id -u`
# export id_in_string=`whoami`

. $PROGDIR/SqoopRunBook.env

# create the hdfs staging directories if not done already
sh $SQOOP_PROJ_DIR/scripts/get_staginghdfs_dir | while read aline
do
        echo $aline
        umask 007

        if hadoop fs -test -d $aline
        then
                :
        else
                hadoop dfs -mkdir $aline
        fi

done

# create the hdfs final directories if not done already
sh $SQOOP_PROJ_DIR/scripts/get_finalhdfs_dir | while read aline
do
        echo $aline
        umask 007

        if hadoop fs -test -d $aline
        then
                :
        else
                hadoop dfs -mkdir $aline
        fi

done

cob_params=`sh $SQOOP_PROJ_DIR/scripts/get_cob_params` 2> /dev/null

cd $TALEND_LOG


###################ETLAutowatch process check start##################################
if [[ "$*" == *"enableAutowatch=Y"* ]]

then

   ETLAutoWatch_Process=`ps -ef | grep readJMSMessages_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

        if  [ -z "$ETLAutoWatch_Process" ]
        then
        LOGFILE=$TALEND_LOG/$id_in_string."wrap_Master.sh".$NOW.log
        touch $LOGFILE
        echo "ETLAutoWatch process is not running.Ensure to start the ETLAutoWatch script using fid bdautowatch using command nohup ./wrap_readJMSMessages_run.sh & located under /opt/cgfiles/common/ETLAutoWatch/ folder" >> $LOGFILE
        exit 1
        fi

fi
####################ETLAutowatch process check end ########################



sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_Master.sh $* ${cob_params}

RC=$?

sleep 1

if [ $RC = 0 ]
then
	sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_SRBReport_run.sh $* ${cob_params}
	RC=$?
	sleep 1
fi
 
# nohup sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_CDPF.sh &

find $TALEND_LOG -name \*.log -mtime +21 -user $id_in_string -exec rm -f {} \;

exit 0

#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

ARGS=($*)

. $PROGDIR/SqoopRunBook.env

MODULE=Master_run
export MODULE

sh $PROGDIR/../scripts/get_sqoop_run_version

LOGFILE=$TALEND_LOG/$id_in_string.$PROG.$NOW.log

touch $LOGFILE

PS_OUT=`ps -ef | grep Master_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

. $SQOOP_PROJ_DIR/scripts/get_mysql_run_server >> $LOGFILE 2>&1

for i in ${ARGS[@]}
do
        if echo "$i" | grep "^runGroup="  > /dev/null
        then
                runGroup=$i
                PS_OUT=`ps -ef | grep Master_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} " | grep "$runGroup"`
                break
        fi
done

if [ -z "$PS_OUT" ]
then
	if [ -z "$mServer" ]
        then
                exit 1
	else
		nohup ksh $SQOOP_PROJ_DIR/Master/Master_run.sh --context_param RunBook_Server=$mServer $* >> $LOGFILE 2>&1
		RC=$?
	fi
else
	sh $SQOOP_PROJ_DIR/TopLevelOrchestration/Long_Processes_Notification.sh
fi

. $SQOOP_PROJ_DIR/TopLevelOrchestration/RunSqoopOrchestration.sh.subfunc

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#wrap_Master.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

ARGS=($*)

. $PROGDIR/SqoopRunBook.env

MODULE=Master_run
export MODULE

sh $PROGDIR/../scripts/get_sqoop_run_version

LOGFILE=$TALEND_LOG/$id_in_string.$PROG.$NOW.log

touch $LOGFILE

PS_OUT=`ps -ef | grep Master_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

. $SQOOP_PROJ_DIR/scripts/get_mysql_run_server >> $LOGFILE 2>&1

for i in ${ARGS[@]}
do
        if echo "$i" | grep "^runGroup="  > /dev/null
        then
                runGroup=$i
                PS_OUT=`ps -ef | grep Master_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} " | grep "$runGroup"`
                break
        fi
done

if [ -z "$PS_OUT" ]
then
	if [ -z "$mServer" ]
        then
                exit 1
	else
		nohup ksh $SQOOP_PROJ_DIR/Master/Master_run.sh --context_param RunBook_Server=$mServer $* >> $LOGFILE 2>&1
		RC=$?
	fi
else
	sh $SQOOP_PROJ_DIR/TopLevelOrchestration/Long_Processes_Notification.sh
fi

. $SQOOP_PROJ_DIR/TopLevelOrchestration/RunSqoopOrchestration.sh.subfunc

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#RunSqoopOrchestration.sh.subfunc
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

RC=$?

if [ $RC = 0 ]
then
        egrep -i "Exception|failed|error" $LOGFILE | grep -v "^fileError:\|/lib/sqoop/lib/sqljdbc.jar\|dfs.encryption.key.provider.uri\|Shuffles\|StandbyException\|RetryInvocationHandler\|.hiverc\|.avsc\|tFileDelete_3"

        if [ $? = 1 ]
        then
                RC=0
                exit $RC
        else
                RC=0
        fi
fi

. $SQOOP_PROJ_DIR/scripts/get_email_addrs

ENV_from_context=`$SQOOP_PROJ_DIR/scripts/get_TRB_env`

/usr/lib/sendmail -t -v <<-!END!
From: $fromemail
To: suresh.chittoor@citi.com,naveen.nookala@citi.com
Cc: suresh.chittoor@citi.com
Subject: $MODULE Failing:`hostname -s`:$ENV_from_context
Return-Path: suresh.chittoor@citi.com
Content-Type: text/html; charset=us-ascii

`cat $LOGFILE | sed 's/^/\<br\>/'`
<BR>
.
!END!

exit $RC
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#RunSqoopOrchestration_w_wait.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

# TALEND_PROJ_DIR=/opt/cgfiles/common/SqoopFrameWork/SqoopFrameWork
#
# export TALEND_PROJ_DIR

# NOW=`date '+%Y%m%d_%H%M%S'`
# export NOW

# JAVA_HOME=/usr/java/jdk1.6.0_31
# export JAVA_HOME

# export PATH=$JAVA_HOME/bin:$PATH

# TALEND_LOG=/var/log/talend
# export TALEND_LOG

# export id_in_num=`id -u`
# export id_in_string=`whoami`

. $PROGDIR/SqoopRunBook.env

# create the hdfs staging directories if not done already

cd $TALEND_LOG

cob_params=`sh $SQOOP_PROJ_DIR/scripts/get_cob_params` 2> /dev/null

###################ETLAutowatch process check start##################################
if [[ "$*" == *"enableAutowatch=Y"* ]]

then

   ETLAutoWatch_Process=`ps -ef | grep readJMSMessages_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

        if  [ -z "$ETLAutoWatch_Process" ]
        then
        LOGFILE=$TALEND_LOG/$id_in_string."wrap_Master.sh".$NOW.log
        touch $LOGFILE
        echo "ETLAutoWatch process is not running.Ensure to start the ETLAutoWatch script using fid bdautowatch using command nohup ./wrap_readJMSMessages_run.sh & located under /opt/cgfiles/common/ETLAutoWatch/ folder" >> $LOGFILE
        exit 1
        fi

fi
####################ETLAutowatch process check end ########################


if [[ "$*" == *"profilingrejectsOnly=tru"* ]]

then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBConnector_run.sh $* ${cob_params}
     RC=$?
     sleep 1

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBCheckTable_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi
     
     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBBuildL1_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBSendReport_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBReportToHive_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBUpdateDPStatus_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

else

sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_Master.sh $* ${cob_params}

RC=$?
echo $RC

sleep 1

sh $SQOOP_PROJ_DIR/TopLevelOrchestration/get_final_status $* ${cob_params}

RC=`expr $? + $RC`
echo $RC

sleep 1

sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_SRBReport_run.sh $* ${cob_params}

RC=`expr $? + $RC`
echo $RC

sleep 1

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBConnector_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi
    
     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBCheckTable_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBBuildL1_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBSendReport_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBReportToHive_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

     if [ $RC -eq 0 ]
     then
     nohup sh $SQOOP_PROJ_DIR/CDPFEngine/TopLevelOrchestration/wrap_CDPF_SRBUpdateDPStatus_run.sh $* ${cob_params}
     RC=$?
     sleep 1
     fi

fi

# nohup sh $SQOOP_PROJ_DIR/TopLevelOrchestration/wrap_CDPF.sh &

find $TALEND_LOG -name \*.log -mtime +21 -user $id_in_string -exec rm -f {} \;

exit $RC

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#wrap_CDPF.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

. $PROGDIR/SqoopRunBook.env

MODULE=Profiling_run
export MODULE

LOGFILE=$TALEND_LOG/$id_in_string.$PROG.$NOW.log

touch $LOGFILE

PS_OUT=`ps -ef | grep run_cdpfsrb.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

if [ -z "$PS_OUT" ]
then
      nohup ksh $SQOOP_PROJ_DIR/CDPF/run_cdpfsrb.sh $* > $LOGFILE 2>&1
#       nohup ksh /opt/cgfiles/common/SqoopRunBook/CDPF/run_cdpfsrb.sh > $LOGFILE 2>&1
else
      sh $SQOOP_PROJ_DIR/TopLevelOrchestration/Long_Processes_Notification.sh
fi

. $SQOOP_PROJ_DIR/TopLevelOrchestration/RunSqoopOrchestration.sh.subfunc
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#wrap_SRBReport_run.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

. $PROGDIR/SqoopRunBook.env

MODULE=SRBReport
export MODULE

LOGFILE=$TALEND_LOG/$id_in_string.$PROG.$NOW.log

touch $LOGFILE

PS_OUT=`ps -ef | grep SRBReport_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

if [ -z "$PS_OUT" ]
then
      nohup ksh $SQOOP_PROJ_DIR/SRBReport/SRBReport_run.sh $* > $LOGFILE 2>&1
else
      sh $SQOOP_PROJ_DIR/TopLevelOrchestration/Long_Processes_Notification.sh
fi

. $SQOOP_PROJ_DIR/TopLevelOrchestration/RunSqoopOrchestration.sh.subfunc

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#wrap_LoadRunBookTemplate.sh
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

#! /bin/sh

export PROG=`basename $0`
export PROGDIR=`dirname $0`

uid=`whoami`
echo $uid

ARGS=($*)

. $PROGDIR/SqoopRunBook.env

MODULE=LoadRunBookTemplate
export MODULE

VERSION=`readlink $SQOOP_PROJ_DIR`
export VERSION

LOGFILE=$TALEND_LOG/$id_in_string.$PROG.$NOW.log 

touch $LOGFILE

echo Version -- $VERSION > $LOGFILE

PS_OUT=`ps -ef | grep LoadRunBookTemplate_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} "`

. $SQOOP_PROJ_DIR/scripts/get_mysql_run_server >> $LOGFILE 2>&1

for i in ${ARGS[@]}
do
	echo $i
        if echo "$i" | grep "^runGroup="  > /dev/null
        then
                runGroup=$i

                PS_OUT=`ps -ef | grep LoadRunBookTemplate_run.sh | egrep -v "grep| $$ " | egrep "^${id_in_num} |^${id_in_string} " | grep "$runGroup"`
                break
        fi
	if echo "$i" | grep "^category="  > /dev/null
	then
		eval "$i"
		folderCategory=${category// /}
	fi
done

if [ -z "$PS_OUT" ]
then
	if [ -z "$mServer" ]
	then
		exit 1
	else
		if [ -z "$category" ]
		then
			nohup sh $SQOOP_PROJ_DIR/LoadRunBook/LoadRunBookTemplate/LoadRunBookTemplate_run.sh --context_param RunBook_Server=$mServer $* >> $LOGFILE 2>&1
		else
			nohup sh $SQOOP_PROJ_DIR/../SharepointRunBook/DownloadTemplate/DownloadTemplate_run.sh --context_param RunBook_Server=$mServer $* >> $LOGFILE 2>&1
			if [ $? = 0 ]
			then
				sh $SQOOP_PROJ_DIR/LoadRunBook/LoadRunBookTemplate/LoadRunBookTemplate_run.sh --context_param RunBook_Server=$mServer --context_param fileDir=/home/$uid/$folderCategory/Spreadsheets $* >> $LOGFILE 2>&1
			fi
		fi
		RC=$?
	fi
else
      	sh $SQOOP_PROJ_DIR/TopLevelOrchestration/Long_Processes_Notification.sh
fi

. $SQOOP_PROJ_DIR/TopLevelOrchestration/RunSqoopOrchestration.sh.subfunc
. $PROGDIR/SqoopRunBook.env
exit $RC

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#SqoopRunBook.env
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


TALEND_PROJ_DIR=/opt/cgfiles/common/TalendRunBook
export TALEND_PROJ_DIR

SQOOP_PROJ_DIR=/opt/cgfiles/common/SqoopRunBook_3.5
export SQOOP_PROJ_DIR

export HADOOP_OPTS=-Djava.security.egd=file:/dev/../dev/urandom

export HADOOP_CONF_DIR=/etc/hive/conf

export CLASSPATH=/opt/cgfiles/common/jdbc/*.jar:$CLASSPATH


NOW=`date '+%Y%m%d_%H%M%S'`
export NOW

JAVA_HOME=/usr/java/latest
export JAVA_HOME

export PATH=$JAVA_HOME/bin:$PATH

TALEND_LOG=/var/log/talend
export TALEND_LOG

export id_in_num=`id -u`
export id_in_string=`whoami`

long_running_threshold=480
export long_running_threshold

export HADOOP_CONF_DIR=/etc/hive/conf
#! /bin/sh

#####################################################################################
# SCRIPT NAME          :   Long_Processes_Notification.sh                           #
# DESCRIPTION          :   It will read process ids and elapsed times from the ps   #
#                          command and determines the process ids which are running #
#                          more than 8 hours and sends an email with HTML Table     #
#                          format.                                                  #
# VERSION              :   1.0.0.0                                                  #
# DATE OF CREATION     :   07-15-2014                                               #
#####################################################################################

minutes_now=`date +%M`
if [ $minutes_now -gt 10 ]
then
        # only let this monitoring run once every hour
    exit 1
fi
export currentTime=$(date +"%Y/%m/%d %H:%M:%S")
export contentFlag="N"
export overAllContent="N"
export currentPath=$(pwd)
export fromEmailAddress="dl.citidata.global.elt.coe.monitoring@imcnam.ssmb.com"
export toEmailAddress="ss55296@imcnam.ssmb.com"
##export WHOAMI=`whoami`
NOW=`date '+%Y%m%d_%H%M%S'`
export attachmentPath=/var/log/talend/Long_Processes_Notification.template.$NOW.html.log

##MODULE=$1

rm -f ${attachmentPath}
touch ${attachmentPath}


send_email()
{
        if [ "X${overAllContent}" == "XY" ]
        then
        (
                echo "From: ${fromEmailAddress} "
                echo "To: ${toEmailAddress} "
                echo "MIME-Version: 1.0"
                echo "Subject: Long running processes at $currentTime"
                echo "Content-Type: text/html"
                cat $attachmentPath
        ) | sendmail -t
        fi
}

htmlAttach()
{
        unset HEADERMAIL
        unset content
        unset TRAILERMAIL
        #MODULE=$1
        moduleName=$MODULE;
        HEADERMAIL=`(echo $'\n'
        echo "Module: $moduleName:"
        echo "<table cellspacing=5 border=1 cellpadding=0><tbody>"
        echo "<tr><td align=center bgcolor=lightblue>ProcessID</td>"
        echo "<td align=center bgcolor=lightblue>Month</td>"
        echo "<td align=center bgcolor=lightblue>Day</td>"
        echo "<td align=center bgcolor=lightblue>Command</td></tr>" )`

#       ps -eo uid,pid,etime,start,cmd | grep $moduleName | awk '{print $2"\t"$4"\t"$5"\t"$7;}' | awk '{
        content=`ps -eo uid,pid,etime,start,cmd | grep $moduleName | awk '{
                        if ($5 ~ /-/)
                        {
                                printf "<tr>";
                                for (i=1; i<NF; i++)
                                {
                                        if( 2 == i || 4 == i || 5 == i )
                                        {
                                            printf "<td align=center bgcolor=ivory>%s</td>\n", $i
                                        }
                                }
                                printf "<td align=center bgcolor=ivory>%s</td>\n", $NF
                                printf "</tr>";
                        }
                        else
                        {
                                split($5,A,":");
                                T=(A[1]*60)+A[2];
                                if (T>480)
                                {
                                        printf "<tr>";
                                        for (i=1; i< NF-1; i++)
                                        {
                                                if( 2 == i || 4 == i || 5 == i )
                                                {
                                                     printf "<td align=center bgcolor=ivory>%s</td>\n", $i
                                                }
                                        }
                                        printf "<td align=center bgcolor=ivory>%s\t%s</td>\n", $(NF-1), $NF
                                        printf "</tr>";
                                }
                        }
                        }'`

        if [ ! -z "$content" ]
                then
                        echo "${HEADERMAIL}${content}${TRAILERMAIL}"  >> ${attachmentPath}
                        contentFlag="Y"
                else
                        contentFlag="N"
                fi

       TRAILERMAIL=`echo "</tbody></table>"`
}

##a[1]=CleanseFiles;
##a[2]=ValidateCleanFiles;
##a[3]=CreateIngestReject;
##a[4]=InsertHDFSHive;

##for i in "${a[@]}"
##do
#       echo "Calling function with: $i";
        contentFlag="N"

        htmlAttach
        if [ "X${contentFlag}" == "XY" ]
        then
            overAllContent="Y"
        fi
##done

#htmlAttach

send_email


# /bin/sh

PROG=`basename $0`
PROGDIR=`dirname $0`

HOST=`hostname -s`

TRB_ENV=`grep "^$HOST|" $PROGDIR/$PROG.env | cut -d "|" -f2`

echo $TRB_ENV

  

